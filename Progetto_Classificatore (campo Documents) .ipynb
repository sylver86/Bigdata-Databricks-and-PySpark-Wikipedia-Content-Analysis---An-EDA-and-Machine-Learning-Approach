{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb815bf-3666-4ee9-974f-5fe911a9f12c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Attività di EDA su Wikipedia\n",
    "\n",
    "## Descrizione del dataset\n",
    "\n",
    "Il dataset offerto è composto da 4 colonne:\n",
    "\n",
    "- **title**: indica il titolo dell'articolo\n",
    "- **summary**: contiene l'introduzione dell'articolo\n",
    "- **documents**: contiene l'articolo completo\n",
    "- **categoria**: contiene la categoria associata all'articolo\n",
    "\n",
    "## Obiettivi\n",
    "\n",
    "### 1. Attività EDA:\n",
    "È necessario svolgere un'attività di EDA per analizzare e valutare statisticamente tutto il contenuto informativo offerto da Wikipedia. Il dataset fornito possiede le seguenti categorie:\n",
    "\n",
    "- 'culture'\n",
    "- 'economics'\n",
    "- 'energy'\n",
    "- 'engineering'\n",
    "- 'finance'\n",
    "- 'humanities'\n",
    "- 'medicine'\n",
    "- 'pets'\n",
    "- 'politics'\n",
    "- 'research'\n",
    "- 'science'\n",
    "- 'sports'\n",
    "- 'technology'\n",
    "- 'trade'\n",
    "- 'transport'\n",
    "\n",
    "Per ogni categoria, calcolare le seguenti informazioni:\n",
    "\n",
    "1. Numero di articoli\n",
    "2. Numero medio di parole utilizzate\n",
    "3. Numero massimo di parole presenti nell'articolo più lungo\n",
    "4. Numero minimo di parole presenti nell'articolo più corto\n",
    "5. Per ogni categoria, individuare la nuvola di parole più rappresentativa\n",
    "\n",
    "\n",
    "### 2. Sviluppo classificatore NLP articoli :\n",
    "\n",
    "Dopo aver svolto l'analisi richiesta, addestrare e testare un classificatore testuale capace di classificare gli articoli (secondo le categorie presenti nel dataset) che saranno in futuro inseriti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7c6594-af1e-40e4-95f7-292774745856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Implementazione Classificatore Testuale :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7601a7bc-ea83-4314-87a9-c2f338952b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "In questa fase del nostro progetto, ci concentreremo su due obiettivi principali sullo sviluppare e valutare un modello di classificazione per articoli futuri\n",
    "\n",
    "Il nostro obiettivo è creare un modello capace di classificare automaticamente nuovi articoli nelle categorie appropriate. <br>\n",
    "Per fare ciò andremo ad eseguire le seguenti fasi implementative :\n",
    "\n",
    "1. Preparazione dei Dati:\n",
    "   - Divideremo il nostro dataset in set di addestramento e di test\n",
    "   - Utilizziamo i dati già vettorizzati in precedenza per avere una rappresentazione numerica adatta al machine learning\n",
    "\n",
    "2. Addestramento del Modello:\n",
    "   - Sceglieremo un Logistic Regression come algoritmo di classificazione appropriato \n",
    "   - Addestreremo il modello sui dati di training\n",
    "\n",
    "3. Valutazione del Modello:\n",
    "   - Testeremo il modello sul set di dati di test\n",
    "   - Valuteremo le performance usando metriche come accuratezza, precisione, recall e F1-score\n",
    "\n",
    "Queste 4 fasi le effettueremo sia sul df \"summary\" che su \"documents\" a fine di verificare quale dei due fornisce le performance migliori.\n",
    "\n",
    "**Risultato Atteso**\n",
    "\n",
    "Al termine di questo processo, avremo un modello di classificazione che sarà in grado di:\n",
    "- Analizzare il contenuto di nuovi articoli\n",
    "- Assegnare a questi articoli la categoria più probabile basandosi sulle caratteristiche apprese dal nostro corpus\n",
    "\n",
    "Questo strumento sarà prezioso per automatizzare la classificazione di futuri articoli, rendendo più efficiente il processo di categorizzazione e organizzazione dei contenuti.\n",
    "\n",
    "Nelle prossime celle di codice, implementeremo questi passaggi utilizzando le funzionalità di machine learning di PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25c79e2-6f23-415d-8d0c-c3825186c2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Iniziamo il progetto andandoci a scaricare il dataset di lavoro :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4ac5b6-af00-4772-af88-a11f8856128d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-13 10:08:41--  https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\r\nResolving proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)... 3.5.204.157, 52.95.155.102\r\nConnecting to proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)|3.5.204.157|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1003477941 (957M) [text/csv]\r\nSaving to: ‘wikipedia.csv.1’\r\n\r\n\rwikipedia.csv.1       0%[                    ]       0  --.-KB/s               \rwikipedia.csv.1       0%[                    ]  66.48K   249KB/s               \rwikipedia.csv.1       0%[                    ] 147.53K   275KB/s               \rwikipedia.csv.1       0%[                    ] 262.36K   326KB/s               \rwikipedia.csv.1       0%[                    ] 612.77K   571KB/s               \rwikipedia.csv.1       0%[                    ]   1.98M  1.47MB/s               \rwikipedia.csv.1       0%[                    ]   4.88M  3.02MB/s               \rwikipedia.csv.1       0%[                    ]   7.93M  4.20MB/s               \rwikipedia.csv.1       1%[                    ]  10.99M  5.09MB/s               \rwikipedia.csv.1       1%[                    ]  14.12M  5.81MB/s               \rwikipedia.csv.1       1%[                    ]  17.27M  6.41MB/s               \rwikipedia.csv.1       2%[                    ]  20.45M  6.90MB/s               \rwikipedia.csv.1       2%[                    ]  23.73M  7.34MB/s    eta 2m 7s  \rwikipedia.csv.1       2%[                    ]  27.13M  7.75MB/s    eta 2m 7s  \rwikipedia.csv.1       3%[                    ]  30.59M  8.12MB/s    eta 2m 7s  \rwikipedia.csv.1       3%[                    ]  34.09M  8.44MB/s    eta 2m 7s  \rwikipedia.csv.1       3%[                    ]  37.67M  8.75MB/s    eta 1m 45s \rwikipedia.csv.1       4%[                    ]  41.42M  9.06MB/s    eta 1m 45s \rwikipedia.csv.1       4%[                    ]  45.21M  9.88MB/s    eta 1m 45s \rwikipedia.csv.1       5%[>                   ]  49.04M  10.7MB/s    eta 1m 45s \rwikipedia.csv.1       5%[>                   ]  52.98M  12.2MB/s    eta 92s    \rwikipedia.csv.1       5%[>                   ]  56.95M  12.8MB/s    eta 92s    \rwikipedia.csv.1       6%[>                   ]  60.73M  13.2MB/s    eta 92s    \rwikipedia.csv.1       6%[>                   ]  64.87M  13.4MB/s    eta 92s    \rwikipedia.csv.1       7%[>                   ]  69.10M  13.6MB/s    eta 83s    \rwikipedia.csv.1       7%[>                   ]  73.38M  14.1MB/s    eta 83s    \rwikipedia.csv.1       8%[>                   ]  77.79M  14.5MB/s    eta 83s    \rwikipedia.csv.1       8%[>                   ]  82.20M  14.5MB/s    eta 83s    \rwikipedia.csv.1       9%[>                   ]  86.63M  14.9MB/s    eta 75s    \rwikipedia.csv.1       9%[>                   ]  91.13M  15.3MB/s    eta 75s    \rwikipedia.csv.1      10%[=>                  ]  95.71M  15.6MB/s    eta 75s    \rwikipedia.csv.1      10%[=>                  ] 100.31M  15.7MB/s    eta 75s    \rwikipedia.csv.1      10%[=>                  ] 105.09M  16.0MB/s    eta 70s    \rwikipedia.csv.1      11%[=>                  ] 109.87M  15.9MB/s    eta 70s    \rwikipedia.csv.1      11%[=>                  ] 114.74M  16.5MB/s    eta 70s    \rwikipedia.csv.1      12%[=>                  ] 119.63M  16.9MB/s    eta 70s    \rwikipedia.csv.1      13%[=>                  ] 124.60M  17.1MB/s    eta 64s    \rwikipedia.csv.1      13%[=>                  ] 129.65M  17.4MB/s    eta 64s    \rwikipedia.csv.1      14%[=>                  ] 134.34M  17.5MB/s    eta 64s    \rwikipedia.csv.1      14%[=>                  ] 139.40M  17.7MB/s    eta 64s    \rwikipedia.csv.1      15%[==>                 ] 144.52M  17.9MB/s    eta 60s    \rwikipedia.csv.1      15%[==>                 ] 149.77M  18.2MB/s    eta 60s    \rwikipedia.csv.1      16%[==>                 ] 155.04M  18.4MB/s    eta 60s    \rwikipedia.csv.1      16%[==>                 ] 160.38M  18.6MB/s    eta 60s    \rwikipedia.csv.1      17%[==>                 ] 165.68M  19.2MB/s    eta 56s    \rwikipedia.csv.1      17%[==>                 ] 168.45M  18.7MB/s    eta 56s    \rwikipedia.csv.1      18%[==>                 ] 173.76M  19.1MB/s    eta 56s    \rwikipedia.csv.1      18%[==>                 ] 179.13M  19.2MB/s    eta 56s    \rwikipedia.csv.1      19%[==>                 ] 184.87M  19.2MB/s    eta 53s    \rwikipedia.csv.1      19%[==>                 ] 190.40M  19.7MB/s    eta 53s    \rwikipedia.csv.1      20%[===>                ] 195.98M  19.7MB/s    eta 53s    \rwikipedia.csv.1      21%[===>                ] 201.88M  20.0MB/s    eta 53s    \rwikipedia.csv.1      21%[===>                ] 207.49M  20.2MB/s    eta 50s    \rwikipedia.csv.1      22%[===>                ] 213.29M  20.2MB/s    eta 50s    \rwikipedia.csv.1      22%[===>                ] 219.18M  20.6MB/s    eta 50s    \rwikipedia.csv.1      23%[===>                ] 224.62M  20.6MB/s    eta 50s    \rwikipedia.csv.1      24%[===>                ] 230.37M  20.9MB/s    eta 47s    \rwikipedia.csv.1      24%[===>                ] 235.35M  20.7MB/s    eta 47s    \rwikipedia.csv.1      25%[====>               ] 240.87M  20.7MB/s    eta 47s    \rwikipedia.csv.1      25%[====>               ] 246.67M  20.9MB/s    eta 47s    \rwikipedia.csv.1      26%[====>               ] 249.63M  21.0MB/s    eta 47s    \rwikipedia.csv.1      26%[====>               ] 255.09M  20.9MB/s    eta 44s    \rwikipedia.csv.1      27%[====>               ] 260.37M  21.3MB/s    eta 44s    \rwikipedia.csv.1      27%[====>               ] 263.87M  21.0MB/s    eta 44s    \rwikipedia.csv.1      28%[====>               ] 268.84M  21.0MB/s    eta 44s    \rwikipedia.csv.1      28%[====>               ] 273.13M  21.4MB/s    eta 44s    \rwikipedia.csv.1      29%[====>               ] 277.57M  21.0MB/s    eta 42s    \rwikipedia.csv.1      29%[====>               ] 281.40M  21.2MB/s    eta 42s    \rwikipedia.csv.1      29%[====>               ] 286.20M  21.2MB/s    eta 42s    \rwikipedia.csv.1      30%[=====>              ] 290.82M  20.9MB/s    eta 42s    \rwikipedia.csv.1      30%[=====>              ] 294.98M  21.3MB/s    eta 42s    \rwikipedia.csv.1      31%[=====>              ] 299.48M  21.4MB/s    eta 40s    \rwikipedia.csv.1      31%[=====>              ] 303.73M  21.4MB/s    eta 40s    \rwikipedia.csv.1      32%[=====>              ] 307.67M  21.4MB/s    eta 40s    \rwikipedia.csv.1      32%[=====>              ] 312.20M  21.4MB/s    eta 40s    \rwikipedia.csv.1      33%[=====>              ] 316.67M  21.5MB/s    eta 40s    \rwikipedia.csv.1      33%[=====>              ] 321.13M  21.4MB/s    eta 38s    \rwikipedia.csv.1      34%[=====>              ] 325.67M  21.6MB/s    eta 38s    \rwikipedia.csv.1      34%[=====>              ] 329.88M  21.3MB/s    eta 38s    \rwikipedia.csv.1      34%[=====>              ] 334.56M  21.4MB/s    eta 38s    \rwikipedia.csv.1      35%[======>             ] 338.90M  21.6MB/s    eta 38s    \rwikipedia.csv.1      35%[======>             ] 343.46M  21.4MB/s    eta 36s    \rwikipedia.csv.1      36%[======>             ] 346.57M  21.2MB/s    eta 36s    \rwikipedia.csv.1      36%[======>             ] 351.73M  21.1MB/s    eta 36s    \rwikipedia.csv.1      37%[======>             ] 355.48M  19.4MB/s    eta 36s    \rwikipedia.csv.1      37%[======>             ] 359.79M  19.2MB/s    eta 35s    \rwikipedia.csv.1      37%[======>             ] 362.88M  18.4MB/s    eta 35s    \rwikipedia.csv.1      38%[======>             ] 366.01M  17.7MB/s    eta 35s    \rwikipedia.csv.1      38%[======>             ] 369.15M  17.2MB/s    eta 35s    \rwikipedia.csv.1      38%[======>             ] 372.35M  16.6MB/s    eta 35s    \rwikipedia.csv.1      39%[======>             ] 375.70M  16.0MB/s    eta 35s    \rwikipedia.csv.1      39%[======>             ] 379.07M  15.6MB/s    eta 35s    \rwikipedia.csv.1      39%[======>             ] 382.52M  14.8MB/s    eta 35s    \rwikipedia.csv.1      40%[=======>            ] 385.96M  14.4MB/s    eta 35s    \rwikipedia.csv.1      40%[=======>            ] 389.48M  13.6MB/s    eta 35s    \rwikipedia.csv.1      41%[=======>            ] 393.06M  13.3MB/s    eta 35s    \rwikipedia.csv.1      41%[=======>            ] 396.74M  12.9MB/s    eta 35s    \rwikipedia.csv.1      41%[=======>            ] 400.52M  12.8MB/s    eta 34s    \rwikipedia.csv.1      42%[=======>            ] 404.46M  12.4MB/s    eta 34s    \rwikipedia.csv.1      42%[=======>            ] 408.43M  13.2MB/s    eta 34s    \rwikipedia.csv.1      43%[=======>            ] 412.56M  13.3MB/s    eta 34s    \rwikipedia.csv.1      43%[=======>            ] 416.81M  13.6MB/s    eta 33s    \rwikipedia.csv.1      44%[=======>            ] 421.15M  14.0MB/s    eta 33s    \rwikipedia.csv.1      44%[=======>            ] 425.56M  14.1MB/s    eta 33s    \rwikipedia.csv.1      44%[=======>            ] 429.99M  14.6MB/s    eta 33s    \rwikipedia.csv.1      45%[========>           ] 434.54M  14.9MB/s    eta 32s    \rwikipedia.csv.1      45%[========>           ] 436.81M  14.4MB/s    eta 32s    \rwikipedia.csv.1      45%[========>           ] 438.45M  13.9MB/s    eta 32s    \rwikipedia.csv.1      45%[========>           ] 439.67M  13.3MB/s    eta 32s    \rwikipedia.csv.1      46%[========>           ] 442.17M  13.0MB/s    eta 32s    \rwikipedia.csv.1      46%[========>           ] 444.79M  12.7MB/s    eta 32s    \rwikipedia.csv.1      46%[========>           ] 447.48M  12.5MB/s    eta 32s    \rwikipedia.csv.1      47%[========>           ] 450.20M  12.3MB/s    eta 32s    \rwikipedia.csv.1      47%[========>           ] 452.96M  12.0MB/s    eta 32s    \rwikipedia.csv.1      47%[========>           ] 455.77M  11.7MB/s    eta 32s    \rwikipedia.csv.1      47%[========>           ] 458.60M  11.4MB/s    eta 32s    \rwikipedia.csv.1      48%[========>           ] 461.48M  11.2MB/s    eta 32s    \rwikipedia.csv.1      48%[========>           ] 464.38M  10.5MB/s    eta 32s    \rwikipedia.csv.1      48%[========>           ] 467.32M  10.5MB/s    eta 32s    \rwikipedia.csv.1      49%[========>           ] 470.21M  9.71MB/s    eta 32s    \rwikipedia.csv.1      49%[========>           ] 473.02M  9.44MB/s    eta 32s    \rwikipedia.csv.1      49%[========>           ] 475.17M  9.16MB/s    eta 32s    \rwikipedia.csv.1      49%[========>           ] 477.35M  9.19MB/s    eta 32s    \rwikipedia.csv.1      50%[=========>          ] 478.88M  9.47MB/s    eta 32s    \rwikipedia.csv.1      50%[=========>          ] 481.82M  9.57MB/s    eta 32s    \rwikipedia.csv.1      50%[=========>          ] 484.10M  9.33MB/s    eta 31s    \rwikipedia.csv.1      50%[=========>          ] 486.42M  9.46MB/s    eta 31s    \rwikipedia.csv.1      51%[=========>          ] 488.74M  9.23MB/s    eta 31s    \rwikipedia.csv.1      51%[=========>          ] 489.95M  9.18MB/s    eta 31s    \rwikipedia.csv.1      51%[=========>          ] 492.31M  8.97MB/s    eta 31s    \rwikipedia.csv.1      51%[=========>          ] 494.70M  9.12MB/s    eta 31s    \rwikipedia.csv.1      51%[=========>          ] 495.93M  8.73MB/s    eta 31s    \rwikipedia.csv.1      52%[=========>          ] 498.34M  8.67MB/s    eta 31s    \rwikipedia.csv.1      52%[=========>          ] 500.77M  8.54MB/s    eta 31s    \rwikipedia.csv.1      52%[=========>          ] 502.93M  8.99MB/s    eta 31s    \rwikipedia.csv.1      52%[=========>          ] 504.48M  8.66MB/s    eta 31s    \rwikipedia.csv.1      52%[=========>          ] 506.71M  8.96MB/s    eta 31s    \rwikipedia.csv.1      53%[=========>          ] 508.21M  8.68MB/s    eta 31s    \rwikipedia.csv.1      53%[=========>          ] 510.70M  9.23MB/s    eta 31s    \rwikipedia.csv.1      53%[=========>          ] 513.10M  9.18MB/s    eta 31s    \rwikipedia.csv.1      53%[=========>          ] 514.43M  8.94MB/s    eta 30s    \rwikipedia.csv.1      53%[=========>          ] 516.76M  9.35MB/s    eta 30s    \rwikipedia.csv.1      54%[=========>          ] 518.23M  9.00MB/s    eta 30s    \rwikipedia.csv.1      54%[=========>          ] 520.71M  9.21MB/s    eta 30s    \rwikipedia.csv.1      54%[=========>          ] 523.02M  9.45MB/s    eta 30s    \rwikipedia.csv.1      54%[=========>          ] 524.51M  9.09MB/s    eta 30s    \rwikipedia.csv.1      55%[==========>         ] 526.88M  9.31MB/s    eta 30s    \rwikipedia.csv.1      55%[==========>         ] 528.43M  9.31MB/s    eta 30s    \rwikipedia.csv.1      55%[==========>         ] 530.70M  9.37MB/s    eta 30s    \rwikipedia.csv.1      55%[==========>         ] 532.24M  9.27MB/s    eta 30s    \rwikipedia.csv.1      55%[==========>         ] 534.54M  9.42MB/s    eta 30s    \rwikipedia.csv.1      56%[==========>         ] 536.04M  9.31MB/s    eta 30s    \rwikipedia.csv.1      56%[==========>         ] 538.35M  9.49MB/s    eta 30s    \rwikipedia.csv.1      56%[==========>         ] 540.40M  9.34MB/s    eta 30s    \rwikipedia.csv.1      56%[==========>         ] 542.15M  9.27MB/s    eta 30s    \rwikipedia.csv.1      56%[==========>         ] 544.54M  9.45MB/s    eta 29s    \rwikipedia.csv.1      57%[==========>         ] 546.09M  9.38MB/s    eta 29s    \rwikipedia.csv.1      57%[==========>         ] 548.42M  9.37MB/s    eta 29s    \rwikipedia.csv.1      57%[==========>         ] 549.87M  9.24MB/s    eta 29s    \rwikipedia.csv.1      57%[==========>         ] 552.21M  9.57MB/s    eta 29s    \rwikipedia.csv.1      57%[==========>         ] 553.95M  9.51MB/s    eta 29s    \rwikipedia.csv.1      58%[==========>         ] 556.02M  9.36MB/s    eta 29s    \rwikipedia.csv.1      58%[==========>         ] 557.66M  9.38MB/s    eta 29s    \rwikipedia.csv.1      58%[==========>         ] 559.84M  9.48MB/s    eta 29s    \rwikipedia.csv.1      58%[==========>         ] 562.15M  9.67MB/s    eta 29s    \rwikipedia.csv.1      58%[==========>         ] 563.65M  9.53MB/s    eta 28s    \rwikipedia.csv.1      59%[==========>         ] 566.01M  9.56MB/s    eta 28s    \rwikipedia.csv.1      59%[==========>         ] 567.48M  9.46MB/s    eta 28s    \rwikipedia.csv.1      59%[==========>         ] 569.87M  9.64MB/s    eta 28s    \rwikipedia.csv.1      59%[==========>         ] 571.34M  9.38MB/s    eta 28s    \rwikipedia.csv.1      59%[==========>         ] 573.73M  9.53MB/s    eta 28s    \rwikipedia.csv.1      60%[===========>        ] 575.27M  9.48MB/s    eta 28s    \rwikipedia.csv.1      60%[===========>        ] 577.65M  9.69MB/s    eta 28s    \rwikipedia.csv.1      60%[===========>        ] 579.20M  9.43MB/s    eta 28s    \rwikipedia.csv.1      60%[===========>        ] 581.57M  9.57MB/s    eta 28s    \rwikipedia.csv.1      60%[===========>        ] 583.13M  9.56MB/s    eta 27s    \rwikipedia.csv.1      61%[===========>        ] 585.57M  9.74MB/s    eta 27s    \rwikipedia.csv.1      61%[===========>        ] 587.18M  9.64MB/s    eta 27s    \rwikipedia.csv.1      61%[===========>        ] 589.61M  9.67MB/s    eta 27s    \rwikipedia.csv.1      61%[===========>        ] 591.21M  9.62MB/s    eta 27s    \rwikipedia.csv.1      62%[===========>        ] 593.65M  9.83MB/s    eta 27s    \rwikipedia.csv.1      62%[===========>        ] 595.31M  9.80MB/s    eta 27s    \rwikipedia.csv.1      62%[===========>        ] 597.76M  9.91MB/s    eta 27s    \rwikipedia.csv.1      62%[===========>        ] 599.54M  9.77MB/s    eta 27s    \rwikipedia.csv.1      62%[===========>        ] 601.96M  10.0MB/s    eta 27s    \rwikipedia.csv.1      63%[===========>        ] 603.86M  10.1MB/s    eta 26s    \rwikipedia.csv.1      63%[===========>        ] 606.24M  10.2MB/s    eta 26s    \rwikipedia.csv.1      63%[===========>        ] 608.42M  10.1MB/s    eta 26s    \rwikipedia.csv.1      63%[===========>        ] 610.63M  10.2MB/s    eta 26s    \rwikipedia.csv.1      64%[===========>        ] 613.38M  10.4MB/s    eta 26s    \rwikipedia.csv.1      64%[===========>        ] 615.23M  10.5MB/s    eta 25s    \rwikipedia.csv.1      64%[===========>        ] 618.10M  10.7MB/s    eta 25s    \rwikipedia.csv.1      64%[===========>        ] 619.90M  10.6MB/s    eta 25s    \rwikipedia.csv.1      65%[============>       ] 622.79M  10.8MB/s    eta 25s    \rwikipedia.csv.1      65%[============>       ] 624.92M  10.8MB/s    eta 25s    \rwikipedia.csv.1      65%[============>       ] 627.62M  11.0MB/s    eta 25s    \rwikipedia.csv.1      65%[============>       ] 630.67M  11.4MB/s    eta 25s    \rwikipedia.csv.1      66%[============>       ] 632.60M  11.3MB/s    eta 25s    \rwikipedia.csv.1      66%[============>       ] 635.73M  11.5MB/s    eta 25s    \rwikipedia.csv.1      66%[============>       ] 637.82M  11.5MB/s    eta 25s    \rwikipedia.csv.1      66%[============>       ] 640.98M  11.9MB/s    eta 24s    \rwikipedia.csv.1      67%[============>       ] 643.15M  12.0MB/s    eta 24s    \rwikipedia.csv.1      67%[============>       ] 646.45M  12.4MB/s    eta 24s    \rwikipedia.csv.1      67%[============>       ] 648.68M  12.2MB/s    eta 24s    \rwikipedia.csv.1      68%[============>       ] 652.10M  12.5MB/s    eta 24s    \rwikipedia.csv.1      68%[============>       ] 654.48M  12.7MB/s    eta 23s    \rwikipedia.csv.1      68%[============>       ] 657.98M  13.1MB/s    eta 23s    \rwikipedia.csv.1      69%[============>       ] 660.49M  13.2MB/s    eta 23s    \rwikipedia.csv.1      69%[============>       ] 664.07M  13.4MB/s    eta 23s    \rwikipedia.csv.1      69%[============>       ] 666.70M  13.5MB/s    eta 23s    \rwikipedia.csv.1      70%[=============>      ] 670.43M  14.0MB/s    eta 21s    \rwikipedia.csv.1      70%[=============>      ] 673.17M  14.1MB/s    eta 21s    \rwikipedia.csv.1      70%[=============>      ] 677.01M  14.6MB/s    eta 21s    \rwikipedia.csv.1      71%[=============>      ] 679.98M  14.5MB/s    eta 21s    \rwikipedia.csv.1      71%[=============>      ] 683.93M  14.9MB/s    eta 21s    \rwikipedia.csv.1      71%[=============>      ] 687.02M  15.3MB/s    eta 20s    \rwikipedia.csv.1      72%[=============>      ] 691.15M  15.8MB/s    eta 20s    \rwikipedia.csv.1      72%[=============>      ] 694.35M  15.9MB/s    eta 20s    \rwikipedia.csv.1      73%[=============>      ] 698.73M  16.3MB/s    eta 20s    \rwikipedia.csv.1      73%[=============>      ] 702.09M  16.5MB/s    eta 20s    \rwikipedia.csv.1      73%[=============>      ] 706.46M  17.0MB/s    eta 18s    \rwikipedia.csv.1      74%[=============>      ] 710.21M  17.3MB/s    eta 18s    \rwikipedia.csv.1      74%[=============>      ] 714.76M  17.9MB/s    eta 18s    \rwikipedia.csv.1      75%[==============>     ] 718.35M  17.9MB/s    eta 18s    \rwikipedia.csv.1      75%[==============>     ] 723.06M  18.3MB/s    eta 18s    \rwikipedia.csv.1      75%[==============>     ] 727.13M  18.6MB/s    eta 17s    \rwikipedia.csv.1      76%[==============>     ] 728.57M  18.1MB/s    eta 17s    \rwikipedia.csv.1      76%[==============>     ] 732.54M  17.9MB/s    eta 17s    \rwikipedia.csv.1      76%[==============>     ] 735.63M  17.3MB/s    eta 17s    \rwikipedia.csv.1      77%[==============>     ] 738.77M  16.9MB/s    eta 16s    \rwikipedia.csv.1      77%[==============>     ] 741.92M  16.5MB/s    eta 16s    \rwikipedia.csv.1      77%[==============>     ] 745.06M  16.1MB/s    eta 16s    \rwikipedia.csv.1      78%[==============>     ] 748.21M  15.8MB/s    eta 16s    \rwikipedia.csv.1      78%[==============>     ] 751.38M  15.3MB/s    eta 15s    \rwikipedia.csv.1      78%[==============>     ] 754.59M  14.7MB/s    eta 15s    \rwikipedia.csv.1      79%[==============>     ] 757.79M  14.4MB/s    eta 15s    \rwikipedia.csv.1      79%[==============>     ] 761.06M  14.1MB/s    eta 15s    \rwikipedia.csv.1      79%[==============>     ] 764.37M  13.4MB/s    eta 14s    \rwikipedia.csv.1      80%[===============>    ] 767.70M  13.0MB/s    eta 14s    \rwikipedia.csv.1      80%[===============>    ] 771.06M  12.8MB/s    eta 14s    \rwikipedia.csv.1      80%[===============>    ] 774.56M  12.2MB/s    eta 14s    \rwikipedia.csv.1      81%[===============>    ] 778.10M  12.0MB/s    eta 13s    \rwikipedia.csv.1      81%[===============>    ] 781.67M  12.4MB/s    eta 13s    \rwikipedia.csv.1      82%[===============>    ] 785.34M  12.4MB/s    eta 13s    \rwikipedia.csv.1      82%[===============>    ] 789.13M  12.3MB/s    eta 13s    \rwikipedia.csv.1      82%[===============>    ] 792.21M  12.5MB/s    eta 12s    \rwikipedia.csv.1      83%[===============>    ] 796.10M  12.7MB/s    eta 12s    \rwikipedia.csv.1      83%[===============>    ] 800.06M  12.6MB/s    eta 12s    \rwikipedia.csv.1      84%[===============>    ] 804.20M  13.1MB/s    eta 12s    \rwikipedia.csv.1      84%[===============>    ] 808.52M  13.4MB/s    eta 11s    \rwikipedia.csv.1      84%[===============>    ] 812.88M  13.5MB/s    eta 11s    \rwikipedia.csv.1      85%[================>   ] 817.37M  14.0MB/s    eta 11s    \rwikipedia.csv.1      85%[================>   ] 821.84M  14.3MB/s    eta 11s    \rwikipedia.csv.1      86%[================>   ] 826.37M  14.4MB/s    eta 10s    \rwikipedia.csv.1      86%[================>   ] 830.98M  14.7MB/s    eta 10s    \rwikipedia.csv.1      87%[================>   ] 835.68M  15.3MB/s    eta 10s    \rwikipedia.csv.1      87%[================>   ] 840.46M  15.6MB/s    eta 10s    \rwikipedia.csv.1      88%[================>   ] 845.35M  15.7MB/s    eta 8s     \rwikipedia.csv.1      88%[================>   ] 850.31M  16.4MB/s    eta 8s     \rwikipedia.csv.1      89%[================>   ] 855.31M  16.8MB/s    eta 8s     \rwikipedia.csv.1      89%[================>   ] 860.35M  17.2MB/s    eta 8s     \rwikipedia.csv.1      90%[=================>  ] 865.65M  17.2MB/s    eta 7s     \rwikipedia.csv.1      91%[=================>  ] 870.93M  17.6MB/s    eta 7s     \rwikipedia.csv.1      91%[=================>  ] 876.27M  18.1MB/s    eta 7s     \rwikipedia.csv.1      92%[=================>  ] 881.63M  18.4MB/s    eta 7s     \rwikipedia.csv.1      92%[=================>  ] 886.82M  18.4MB/s    eta 5s     \rwikipedia.csv.1      93%[=================>  ] 892.27M  18.9MB/s    eta 5s     \rwikipedia.csv.1      93%[=================>  ] 897.77M  18.9MB/s    eta 5s     \rwikipedia.csv.1      94%[=================>  ] 903.27M  19.3MB/s    eta 5s     \rwikipedia.csv.1      94%[=================>  ] 908.56M  19.3MB/s    eta 3s     \rwikipedia.csv.1      95%[==================> ] 913.92M  19.6MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 919.60M  19.8MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 925.35M  20.1MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 931.02M  20.1MB/s    eta 2s     \rwikipedia.csv.1      97%[==================> ] 936.52M  20.4MB/s    eta 2s     \rwikipedia.csv.1      98%[==================> ] 942.31M  20.3MB/s    eta 2s     \rwikipedia.csv.1      99%[==================> ] 947.82M  20.6MB/s    eta 2s     \rwikipedia.csv.1      99%[==================> ] 953.15M  20.3MB/s    eta 0s     \rwikipedia.csv.1     100%[===================>] 956.99M  20.4MB/s    in 67s     \r\n\r\n2024-08-13 10:09:49 (14.3 MB/s) - ‘wikipedia.csv.1’ saved [1003477941/1003477941]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b0a872-fe88-4bc8-aa14-310761cc6d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ci predisponiamo il Dataframe spark di lavoro seguendo il seguente codice che va a leggere da un cluster AWS S3 il file csv e lo importa in un dataframe spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127e9378-64ce-496a-b5b3-a67b7c041575",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "N.b:\n",
    "Nel codice che segue, eseguiremo un campionamento stratificato del nostro dataset. Questo approccio è necessario per due ragioni principali poiché in fase di sviluppo stiamo lavorando in un ambiente Databricks Community, pertanto avremo accesso a risorse computazionali limitate.\n",
    "\n",
    "**Processo di campionamento**\n",
    "\n",
    "1. Estrarremo un campione bilanciato del dataset originale.\n",
    "2. Utilizzeremo un metodo di campionamento stratificato basato sulla colonna \"categoria\".\n",
    "3. Questo significa che la distribuzione delle categorie nel nostro campione sarà proporzionalmente la stessa del dataset originale.\n",
    "\n",
    "Questo approccio ci permetterà di lavorare con un dataset più piccolo e gestibile, mantenendo allo stesso tempo la rappresentatività dei nostri dati originali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182dad7f-e652-4268-8a3a-945f654fa5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('/databricks/driver/wikipedia.csv')\n",
    "categoria_col = 'categoria'  \n",
    "\n",
    "# Riduciamo la size per agevolare il running nella versione community di databricks effettuatndo un campionamento stratificato (in modo che si mantiene bilanciato)\n",
    "dataset_sample, _ = train_test_split(dataset, test_size=0.0001, stratify=dataset[categoria_col], random_state=42)\n",
    "\n",
    "spark_df_ = spark.createDataFrame(dataset_sample)\n",
    "spark_df_ = spark_df_.drop(\"Unnamed: 0\")\n",
    "spark_df_.write.mode(\"overwrite\").saveAsTable(\"wikipedia\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbd1eb8-422e-4b03-9c3e-87221253909c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mostriamo il contenuto del dataframe spark di lavoro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d4765-a347-4df9-a149-03448a7f3f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+\n|               title|             summary|           documents|  categoria|\n+--------------------+--------------------+--------------------+-----------+\n|   leonard b. strang|leonard birnie st...|leonard birnie st...|   research|\n|     pascal vasselon|pascal vasselon (...|pascal vasselon (...|engineering|\n|       nada stotland|nada logan stotla...|nada logan stotla...|   medicine|\n|  rikkie-lee tyrrell|rikkie-lee tyrrel...|rikkie-lee tyrrel...|   politics|\n|pauline hanson's ...|pauline hanson's ...|pauline hanson's ...|   politics|\n|2019 chilean air ...|on 9 december 201...|on 9 december 201...|    science|\n|               pirna|pirna (german: [ˈ...|pirna (german: [ˈ...|engineering|\n|            sam cox |samuel victor cox...|samuel victor cox...|   politics|\n|   antiochis of tlos|antiochis of tlos...|antiochis of tlos...|   medicine|\n|castle terrace, c...|castle terrace, o...|castle terrace, o...|engineering|\n|      gregory kealey|gregory s. kealey...|gregory s. kealey...|  economics|\n|  fabrizio zilibotti|fabrizio zilibott...|fabrizio zilibott...|  economics|\n|      bruce sterling|michael bruce ste...|michael bruce ste...| technology|\n|        yana mintoff|yana bland or yan...|yana bland or yan...|  economics|\n|lockheed martin p...|the lockheed mart...|the lockheed mart...|    science|\n|      ruth coppinger|ruth coppinger (b...|ruth coppinger (b...|  economics|\n|aeritalia f-104s ...|the aeritalia f-1...|the aeritalia f-1...|    science|\n|william bell dins...|william bell dins...|william bell dins...| humanities|\n|frederick agnew gill|captain frederick...|captain frederick...|     sports|\n|         tylochromis|tylochromis is a ...|tylochromis is a ...|       pets|\n+--------------------+--------------------+--------------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark_df_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a339b576-b6e5-48a6-a85c-d28f730bf354",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### b. Modello per campo \"Documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac4c8c7-5da7-4176-baa5-1b2ffe2d59cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Procedo a crearmi il Classificatore ed addestrarlo per la colonna \"documents\"; anche in questo caso mostriamo le performance del modello a fine di confrontarlo con il precedente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1180252-8834-499c-97a1-ade79031d9eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Come già fatto in precedenza per il campo \"summary\" andiamo ad effettuare anche per il campo \"documents\" l'attività di Data Cleaning con un apposita Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993e3854-b36e-44ea-83b7-b2494e2d2959",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[ / ]\r[ — ]\r[OK!]\nstopwords_en download started this may take some time.\nApproximate size to download 2.9 KB\n\r[ | ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import concat_ws, col, lit, split\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Definiamo la pipeline di Spark NLP che pulisce il testo che agisce su \"documents\".\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"documents\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setCleanupPatterns([\"[^\\w\\s]\"]) \\\n",
    "    .setLowercase(True) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"normalized\"]).setOutputCol(\"lemma\")\n",
    "stopwords_cleaner = StopWordsCleaner.pretrained().setInputCols([\"lemma\"]).setOutputCol(\"cleaned\")\n",
    "\n",
    "# Crea il pipeline\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer, \n",
    "    normalizer, \n",
    "    lemmatizer, \n",
    "    stopwords_cleaner\n",
    "])\n",
    "\n",
    "# Funzione per applicare la pipeline e pulire il testo\n",
    "def clean_text_spark_nlp(df, input_col=\"summary\", output_col=\"cleaned_text\"):\n",
    "    # Fit della pipeline \n",
    "    fitted_pipeline = nlp_pipeline.fit(df)\n",
    "    \n",
    "    # Applica il pipeline al DataFrame\n",
    "    result = fitted_pipeline.transform(df)\n",
    "\n",
    "    # Estraiamo il testo pulito in una nuova colonna\n",
    "    result_with_clean_text = result.withColumn(output_col, concat_ws(\" \", \"cleaned.result\"))\n",
    "\n",
    "    # Restituiamo il dataframe con la selezione delle colonne originali e la nuova colonna di testo pulito\n",
    "    return result_with_clean_text.select(df.columns + [output_col])\n",
    "\n",
    "# Applichiamo la funzione al tuo DataFrame\n",
    "df_cleaned_documents = clean_text_spark_nlp(spark_df_)\n",
    "\n",
    "# Tokenizziamo la colonna \"cleaned_text\" \n",
    "df_cleaned_documents = df_cleaned_documents.withColumn(\"words\", split(df_cleaned_documents.cleaned_text, \"\\\\s+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149d440f-d228-4584-a632-9f2038bd075b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Quello che ora dobbiamo fare analogamente per quanto fatto su \"summary\" sarà andare a definire la vettorizzazione tramite HashingTF più efficace del CountVectorizer in quanto mappa direttamente le parole in indici del vettore delle feature usando una funzione di hash. Questo riduce il tempo e la memoria necessari per costruire un vocabolario esplicito, come invece avviene con l'ausilio del  CountVectorizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b90721-69fb-44b6-b48a-87adfa56ed62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# Definiamo HashingTF sulla colonna \"words\"\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"word_vector\")  # Impostiamo il numero di feature\n",
    "\n",
    "# Applico la trasformazione\n",
    "df_cleaned_documents_cont_vect = hashingTF.transform(df_cleaned_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90037f3-8980-44df-8045-d4316cb2e09f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Procediamo adesso con l'implementazione ed addestramento del modello stavolta per il campo \"documents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c8225a-1e98-438d-9f7d-553b57fdb701",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Andiamo innanzitutto a codificarci le \"categorie\" per impostare il dataset all'addestramento del modello e poi lo dividiamo in train e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0971459-8f22-4b08-9c13-e090e6611a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from itertools import chain\n",
    "\n",
    "def custom_category_indexer(df, input_col, output_col):\n",
    "    # Ottieni tutte le categorie uniche\n",
    "    categories = df.select(input_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Crea un dizionario di mapping categoria -> indice\n",
    "    category_dict = {cat: idx for idx, cat in enumerate(categories)}\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def optimized_category_indexer(df, input_col, output_col):\n",
    "    # Crea un DataFrame con categorie uniche e indici\n",
    "    category_df = df.select(input_col).distinct()\n",
    "    \n",
    "    # Usa row_number per assegnare indici univoci\n",
    "    window = Window.orderBy(monotonically_increasing_id())\n",
    "    category_df = category_df.withColumn(output_col, row_number().over(window) - 1)\n",
    "    \n",
    "    # Esegui un join per assegnare gli indici al DataFrame originale\n",
    "    df_indexed = df.join(category_df, on=input_col, how=\"left\")\n",
    "    \n",
    "    return df_indexed.withColumn(output_col, col(output_col).cast(IntegerType()))\n",
    "\n",
    "# Uso della funzione\n",
    "df_cleaned_documents_cont_vect = optimized_category_indexer(df_cleaned_documents_cont_vect, \"categoria\", \"categoriaIndex\")\n",
    "\n",
    "# Dividiamo il dataset in training e test\n",
    "train, test = df_cleaned_documents_cont_vect.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9caeb3-1daf-4970-b68a-45d72f65b951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:729)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:447)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:447)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1272)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:987)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:947)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:787)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:803)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:867)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:660)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:729)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:447)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:447)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1272)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:987)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:947)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:787)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:803)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:867)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:660)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cleaned_documents_cont_vect.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74e5e66-f78d-45b1-88f8-4efdf1d47ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Avendo diviso in train e test il dataset e codificato la variabile target procediamo adesso all'implementazione e all'addestramento del modello Logistic Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92127a2-7da7-4537-a842-e1881052ca84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "\n",
    "# Modello Naive Bayes\n",
    "lr = LogisticRegression(featuresCol=\"word_vector\", labelCol=\"categoriaIndex\", maxIter=3)\n",
    "model_documents = lr.fit(train)\n",
    "\n",
    "\n",
    "# Predizioni\n",
    "predictions_documents = model_documents.transform(train)\n",
    "\n",
    "\n",
    "# Valutiamo il modello con la colonna corretta per la classificazione\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoriaIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_documents)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Mostriamo alcune predizioni, usando le colonne corrette\n",
    "predictions_documents.select(\"categoria\", \"categoriaIndex\", \"prediction\", \"probability\").show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Progetto_Classificatore (campo Documents) ",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
