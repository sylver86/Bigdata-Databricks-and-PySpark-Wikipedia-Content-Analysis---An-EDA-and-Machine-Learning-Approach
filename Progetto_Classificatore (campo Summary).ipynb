{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb815bf-3666-4ee9-974f-5fe911a9f12c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Attività di EDA su Wikipedia\n",
    "\n",
    "## Descrizione del dataset\n",
    "\n",
    "Il dataset offerto è composto da 4 colonne:\n",
    "\n",
    "- **title**: indica il titolo dell'articolo\n",
    "- **summary**: contiene l'introduzione dell'articolo\n",
    "- **documents**: contiene l'articolo completo\n",
    "- **categoria**: contiene la categoria associata all'articolo\n",
    "\n",
    "## Obiettivi\n",
    "\n",
    "### 1. Attività EDA:\n",
    "È necessario svolgere un'attività di EDA per analizzare e valutare statisticamente tutto il contenuto informativo offerto da Wikipedia. Il dataset fornito possiede le seguenti categorie:\n",
    "\n",
    "- 'culture'\n",
    "- 'economics'\n",
    "- 'energy'\n",
    "- 'engineering'\n",
    "- 'finance'\n",
    "- 'humanities'\n",
    "- 'medicine'\n",
    "- 'pets'\n",
    "- 'politics'\n",
    "- 'research'\n",
    "- 'science'\n",
    "- 'sports'\n",
    "- 'technology'\n",
    "- 'trade'\n",
    "- 'transport'\n",
    "\n",
    "Per ogni categoria, calcolare le seguenti informazioni:\n",
    "\n",
    "1. Numero di articoli\n",
    "2. Numero medio di parole utilizzate\n",
    "3. Numero massimo di parole presenti nell'articolo più lungo\n",
    "4. Numero minimo di parole presenti nell'articolo più corto\n",
    "5. Per ogni categoria, individuare la nuvola di parole più rappresentativa\n",
    "\n",
    "\n",
    "### 2. Sviluppo classificatore NLP articoli :\n",
    "\n",
    "Dopo aver svolto l'analisi richiesta, addestrare e testare un classificatore testuale capace di classificare gli articoli (secondo le categorie presenti nel dataset) che saranno in futuro inseriti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7c6594-af1e-40e4-95f7-292774745856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Implementazione Classificatore Testuale :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7601a7bc-ea83-4314-87a9-c2f338952b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "In questa fase del nostro progetto, ci concentreremo su due obiettivi principali sullo sviluppare e valutare un modello di classificazione per articoli futuri\n",
    "\n",
    "Il nostro obiettivo è creare un modello capace di classificare automaticamente nuovi articoli nelle categorie appropriate. <br>\n",
    "Per fare ciò andremo ad eseguire le seguenti fasi implementative :\n",
    "\n",
    "1. Preparazione dei Dati:\n",
    "   - Divideremo il nostro dataset in set di addestramento e di test\n",
    "   - Utilizziamo i dati vettorizzati per avere una rappresentazione numerica adatta al machine learning\n",
    "\n",
    "2. Addestramento del Modello:\n",
    "   - Sceglieremo un Logistic Regression come algoritmo di classificazione appropriato \n",
    "   - Addestreremo il modello sui dati di training\n",
    "\n",
    "3. Valutazione del Modello:\n",
    "   - Testeremo il modello sul set di dati di test\n",
    "   - Valuteremo le performance usando metriche come accuratezza, precisione, recall e F1-score\n",
    "\n",
    "Queste 4 fasi le effettueremo sia sul campo \"summary\" che sul campo \"documents\" a fine di verificare quale dei due fornisce le performance migliori in riferimento alle categorie che saranno la nostra variabile target.\n",
    "\n",
    "**Risultato Atteso**\n",
    "\n",
    "Al termine di questo processo, avremo un modello di classificazione che sarà in grado di:\n",
    "- Analizzare il contenuto di nuovi articoli\n",
    "- Assegnare a questi articoli la categoria più probabile basandosi sulle caratteristiche apprese dal nostro corpus\n",
    "\n",
    "Questo strumento sarà prezioso per automatizzare la classificazione di futuri articoli, rendendo più efficiente il processo di categorizzazione e organizzazione dei contenuti.\n",
    "\n",
    "Nelle prossime celle di codice, implementeremo questi passaggi utilizzando le funzionalità di machine learning di PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25c79e2-6f23-415d-8d0c-c3825186c2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Iniziamo il progetto andandoci a scaricare il dataset di lavoro :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4ac5b6-af00-4772-af88-a11f8856128d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-10 12:38:06--  https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\r\nResolving proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)... 3.5.225.182, 52.95.154.36\r\nConnecting to proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)|3.5.225.182|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1003477941 (957M) [text/csv]\r\nSaving to: ‘wikipedia.csv.1’\r\n\r\n\rwikipedia.csv.1       0%[                    ]       0  --.-KB/s               \rwikipedia.csv.1       0%[                    ]  58.59K   220KB/s               \rwikipedia.csv.1       0%[                    ] 109.59K   206KB/s               \rwikipedia.csv.1       0%[                    ] 175.27K   219KB/s               \rwikipedia.csv.1       0%[                    ] 241.06K   226KB/s               \rwikipedia.csv.1       0%[                    ] 332.10K   249KB/s               \rwikipedia.csv.1       0%[                    ] 417.42K   261KB/s               \rwikipedia.csv.1       0%[                    ] 508.46K   273KB/s               \rwikipedia.csv.1       0%[                    ] 623.51K   293KB/s               \rwikipedia.csv.1       0%[                    ] 742.61K   310KB/s               \rwikipedia.csv.1       0%[                    ] 866.66K   326KB/s               \rwikipedia.csv.1       0%[                    ]   1018K   347KB/s               \rwikipedia.csv.1       0%[                    ]   1.16M   371KB/s    eta 44m 0s \rwikipedia.csv.1       0%[                    ]   1.32M   391KB/s    eta 44m 0s \rwikipedia.csv.1       0%[                    ]   1.50M   419KB/s    eta 44m 0s \rwikipedia.csv.1       0%[                    ]   1.70M   447KB/s    eta 44m 0s \rwikipedia.csv.1       0%[                    ]   1.83M   456KB/s    eta 44m 0s \rwikipedia.csv.1       0%[                    ]   2.07M   483KB/s    eta 33m 43s\rwikipedia.csv.1       0%[                    ]   2.34M   516KB/s    eta 33m 43s\rwikipedia.csv.1       0%[                    ]   2.63M   566KB/s    eta 33m 43s\rwikipedia.csv.1       0%[                    ]   2.96M   628KB/s    eta 33m 43s\rwikipedia.csv.1       0%[                    ]   3.32M   722KB/s    eta 26m 3s \rwikipedia.csv.1       0%[                    ]   3.71M   792KB/s    eta 26m 3s \rwikipedia.csv.1       0%[                    ]   4.16M   909KB/s    eta 26m 3s \rwikipedia.csv.1       0%[                    ]   4.64M   982KB/s    eta 26m 3s \rwikipedia.csv.1       0%[                    ]   5.16M  1.06MB/s    eta 20m 2s \rwikipedia.csv.1       0%[                    ]   5.75M  1.21MB/s    eta 20m 2s \rwikipedia.csv.1       0%[                    ]   6.38M  1.34MB/s    eta 20m 2s \rwikipedia.csv.1       0%[                    ]   7.10M  1.44MB/s    eta 20m 2s \rwikipedia.csv.1       0%[                    ]   7.88M  1.64MB/s    eta 15m 12s\rwikipedia.csv.1       0%[                    ]   8.74M  1.79MB/s    eta 15m 12s\rwikipedia.csv.1       1%[                    ]   9.70M  2.01MB/s    eta 15m 12s\rwikipedia.csv.1       1%[                    ]  10.75M  2.17MB/s    eta 15m 12s\rwikipedia.csv.1       1%[                    ]  11.90M  2.40MB/s    eta 11m 26s\rwikipedia.csv.1       1%[                    ]  13.18M  2.69MB/s    eta 11m 26s\rwikipedia.csv.1       1%[                    ]  14.59M  2.90MB/s    eta 11m 26s\rwikipedia.csv.1       1%[                    ]  16.15M  3.25MB/s    eta 11m 26s\rwikipedia.csv.1       1%[                    ]  17.85M  3.53MB/s    eta 8m 31s \rwikipedia.csv.1       2%[                    ]  19.73M  4.02MB/s    eta 8m 31s \rwikipedia.csv.1       2%[                    ]  21.26M  4.37MB/s    eta 8m 31s \rwikipedia.csv.1       2%[                    ]  22.93M  4.59MB/s    eta 8m 31s \rwikipedia.csv.1       2%[                    ]  25.32M  5.23MB/s    eta 8m 31s \rwikipedia.csv.1       2%[                    ]  27.27M  5.69MB/s    eta 6m 8s  \rwikipedia.csv.1       3%[                    ]  29.40M  5.99MB/s    eta 6m 8s  \rwikipedia.csv.1       3%[                    ]  31.74M  6.53MB/s    eta 6m 8s  \rwikipedia.csv.1       3%[                    ]  33.96M  6.87MB/s    eta 6m 8s  \rwikipedia.csv.1       3%[                    ]  36.99M  7.71MB/s    eta 6m 8s  \rwikipedia.csv.1       4%[                    ]  39.17M  7.99MB/s    eta 4m 38s \rwikipedia.csv.1       4%[                    ]  41.65M  8.45MB/s    eta 4m 38s \rwikipedia.csv.1       4%[                    ]  44.70M  9.09MB/s    eta 4m 38s \rwikipedia.csv.1       4%[                    ]  46.31M  9.38MB/s    eta 4m 38s \rwikipedia.csv.1       5%[>                   ]  49.37M  9.80MB/s    eta 4m 38s \rwikipedia.csv.1       5%[>                   ]  51.01M  10.2MB/s    eta 3m 49s \rwikipedia.csv.1       5%[>                   ]  54.12M  10.5MB/s    eta 3m 49s \rwikipedia.csv.1       5%[>                   ]  56.09M  10.7MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  58.87M  11.1MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  61.98M  11.4MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  63.68M  11.4MB/s    eta 3m 16s \rwikipedia.csv.1       6%[>                   ]  66.84M  11.8MB/s    eta 3m 16s \rwikipedia.csv.1       7%[>                   ]  68.59M  12.0MB/s    eta 3m 16s \rwikipedia.csv.1       7%[>                   ]  71.68M  11.9MB/s    eta 3m 16s \rwikipedia.csv.1       7%[>                   ]  73.90M  11.7MB/s    eta 3m 16s \rwikipedia.csv.1       7%[>                   ]  76.52M  11.9MB/s    eta 2m 52s \rwikipedia.csv.1       8%[>                   ]  79.04M  12.1MB/s    eta 2m 52s \rwikipedia.csv.1       8%[>                   ]  81.40M  11.8MB/s    eta 2m 52s \rwikipedia.csv.1       8%[>                   ]  84.57M  12.0MB/s    eta 2m 52s \rwikipedia.csv.1       9%[>                   ]  86.38M  11.8MB/s    eta 2m 52s \rwikipedia.csv.1       9%[>                   ]  89.68M  12.3MB/s    eta 2m 35s \rwikipedia.csv.1       9%[>                   ]  91.46M  12.2MB/s    eta 2m 35s \rwikipedia.csv.1       9%[>                   ]  94.73M  12.3MB/s    eta 2m 35s \rwikipedia.csv.1      10%[=>                  ]  96.59M  12.1MB/s    eta 2m 35s \rwikipedia.csv.1      10%[=>                  ]  99.95M  12.4MB/s    eta 2m 35s \rwikipedia.csv.1      10%[=>                  ] 102.08M  12.4MB/s    eta 2m 23s \rwikipedia.csv.1      10%[=>                  ] 104.68M  12.4MB/s    eta 2m 23s \rwikipedia.csv.1      11%[=>                  ] 107.65M  12.5MB/s    eta 2m 23s \rwikipedia.csv.1      11%[=>                  ] 110.35M  12.6MB/s    eta 2m 23s \rwikipedia.csv.1      11%[=>                  ] 113.09M  12.8MB/s    eta 2m 23s \rwikipedia.csv.1      12%[=>                  ] 115.65M  12.6MB/s    eta 2m 12s \rwikipedia.csv.1      12%[=>                  ] 118.93M  12.8MB/s    eta 2m 12s \rwikipedia.csv.1      12%[=>                  ] 121.04M  12.7MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 124.54M  13.2MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 126.52M  13.1MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 130.10M  13.2MB/s    eta 2m 1s  \rwikipedia.csv.1      13%[=>                  ] 132.74M  13.2MB/s    eta 2m 1s  \rwikipedia.csv.1      14%[=>                  ] 135.63M  13.2MB/s    eta 2m 1s  \rwikipedia.csv.1      14%[=>                  ] 138.85M  13.6MB/s    eta 2m 1s  \rwikipedia.csv.1      14%[=>                  ] 141.17M  13.3MB/s    eta 2m 1s  \rwikipedia.csv.1      15%[==>                 ] 144.51M  13.5MB/s    eta 1m 53s \rwikipedia.csv.1      15%[==>                 ] 146.70M  13.3MB/s    eta 1m 53s \rwikipedia.csv.1      15%[==>                 ] 150.04M  13.7MB/s    eta 1m 53s \rwikipedia.csv.1      15%[==>                 ] 152.23M  13.6MB/s    eta 1m 53s \rwikipedia.csv.1      16%[==>                 ] 155.79M  13.7MB/s    eta 1m 53s \rwikipedia.csv.1      16%[==>                 ] 157.84M  13.6MB/s    eta 1m 47s \rwikipedia.csv.1      16%[==>                 ] 161.37M  13.8MB/s    eta 1m 47s \rwikipedia.csv.1      17%[==>                 ] 163.42M  13.8MB/s    eta 1m 47s \rwikipedia.csv.1      17%[==>                 ] 166.20M  13.7MB/s    eta 1m 47s \rwikipedia.csv.1      17%[==>                 ] 169.09M  13.7MB/s    eta 1m 47s \rwikipedia.csv.1      18%[==>                 ] 172.52M  13.9MB/s    eta 1m 41s \rwikipedia.csv.1      18%[==>                 ] 175.34M  14.0MB/s    eta 1m 41s \rwikipedia.csv.1      18%[==>                 ] 178.17M  13.9MB/s    eta 1m 41s \rwikipedia.csv.1      18%[==>                 ] 181.14M  13.9MB/s    eta 1m 41s \rwikipedia.csv.1      19%[==>                 ] 183.99M  14.0MB/s    eta 1m 41s \rwikipedia.csv.1      19%[==>                 ] 187.04M  14.2MB/s    eta 95s    \rwikipedia.csv.1      19%[==>                 ] 189.88M  14.2MB/s    eta 95s    \rwikipedia.csv.1      20%[===>                ] 192.79M  14.0MB/s    eta 95s    \rwikipedia.csv.1      20%[===>                ] 195.82M  14.1MB/s    eta 95s    \rwikipedia.csv.1      20%[===>                ] 199.37M  14.3MB/s    eta 95s    \rwikipedia.csv.1      21%[===>                ] 201.82M  14.3MB/s    eta 91s    \rwikipedia.csv.1      21%[===>                ] 205.51M  14.5MB/s    eta 91s    \rwikipedia.csv.1      21%[===>                ] 207.82M  14.3MB/s    eta 91s    \rwikipedia.csv.1      22%[===>                ] 211.52M  14.6MB/s    eta 91s    \rwikipedia.csv.1      22%[===>                ] 214.27M  14.6MB/s    eta 91s    \rwikipedia.csv.1      22%[===>                ] 217.55M  14.8MB/s    eta 86s    \rwikipedia.csv.1      23%[===>                ] 220.42M  14.9MB/s    eta 86s    \rwikipedia.csv.1      23%[===>                ] 223.63M  14.8MB/s    eta 86s    \rwikipedia.csv.1      23%[===>                ] 226.63M  14.9MB/s    eta 86s    \rwikipedia.csv.1      24%[===>                ] 229.82M  15.0MB/s    eta 86s    \rwikipedia.csv.1      24%[===>                ] 232.71M  15.1MB/s    eta 82s    \rwikipedia.csv.1      24%[===>                ] 235.92M  14.9MB/s    eta 82s    \rwikipedia.csv.1      25%[====>               ] 239.38M  15.0MB/s    eta 82s    \rwikipedia.csv.1      25%[====>               ] 241.99M  15.1MB/s    eta 82s    \rwikipedia.csv.1      25%[====>               ] 245.57M  15.2MB/s    eta 82s    \rwikipedia.csv.1      25%[====>               ] 248.06M  15.1MB/s    eta 78s    \rwikipedia.csv.1      26%[====>               ] 251.60M  15.1MB/s    eta 78s    \rwikipedia.csv.1      26%[====>               ] 254.19M  15.0MB/s    eta 78s    \rwikipedia.csv.1      26%[====>               ] 257.82M  15.2MB/s    eta 78s    \rwikipedia.csv.1      27%[====>               ] 260.29M  15.1MB/s    eta 78s    \rwikipedia.csv.1      27%[====>               ] 263.92M  15.2MB/s    eta 74s    \rwikipedia.csv.1      27%[====>               ] 266.42M  15.0MB/s    eta 74s    \rwikipedia.csv.1      28%[====>               ] 270.07M  15.2MB/s    eta 74s    \rwikipedia.csv.1      28%[====>               ] 272.71M  15.2MB/s    eta 74s    \rwikipedia.csv.1      28%[====>               ] 276.29M  15.4MB/s    eta 74s    \rwikipedia.csv.1      29%[====>               ] 279.06M  15.2MB/s    eta 71s    \rwikipedia.csv.1      29%[====>               ] 282.55M  15.3MB/s    eta 71s    \rwikipedia.csv.1      29%[====>               ] 285.49M  15.3MB/s    eta 71s    \rwikipedia.csv.1      30%[=====>              ] 288.82M  15.5MB/s    eta 71s    \rwikipedia.csv.1      30%[=====>              ] 291.83M  15.5MB/s    eta 71s    \rwikipedia.csv.1      30%[=====>              ] 295.12M  15.7MB/s    eta 68s    \rwikipedia.csv.1      31%[=====>              ] 298.06M  15.4MB/s    eta 68s    \rwikipedia.csv.1      31%[=====>              ] 301.45M  15.5MB/s    eta 68s    \rwikipedia.csv.1      31%[=====>              ] 304.60M  15.7MB/s    eta 68s    \rwikipedia.csv.1      32%[=====>              ] 307.84M  15.6MB/s    eta 68s    \rwikipedia.csv.1      32%[=====>              ] 311.09M  15.7MB/s    eta 65s    \rwikipedia.csv.1      32%[=====>              ] 314.33M  15.7MB/s    eta 65s    \rwikipedia.csv.1      33%[=====>              ] 317.52M  15.8MB/s    eta 65s    \rwikipedia.csv.1      33%[=====>              ] 320.82M  15.9MB/s    eta 65s    \rwikipedia.csv.1      33%[=====>              ] 324.10M  15.9MB/s    eta 65s    \rwikipedia.csv.1      34%[=====>              ] 327.37M  15.9MB/s    eta 62s    \rwikipedia.csv.1      34%[=====>              ] 330.88M  16.0MB/s    eta 62s    \rwikipedia.csv.1      34%[=====>              ] 333.90M  15.9MB/s    eta 62s    \rwikipedia.csv.1      35%[======>             ] 337.48M  16.1MB/s    eta 62s    \rwikipedia.csv.1      35%[======>             ] 340.62M  16.1MB/s    eta 62s    \rwikipedia.csv.1      35%[======>             ] 344.18M  16.2MB/s    eta 59s    \rwikipedia.csv.1      36%[======>             ] 347.26M  16.2MB/s    eta 59s    \rwikipedia.csv.1      36%[======>             ] 350.85M  16.2MB/s    eta 59s    \rwikipedia.csv.1      36%[======>             ] 353.90M  16.3MB/s    eta 59s    \rwikipedia.csv.1      37%[======>             ] 357.63M  16.5MB/s    eta 59s    \rwikipedia.csv.1      37%[======>             ] 360.63M  16.3MB/s    eta 57s    \rwikipedia.csv.1      38%[======>             ] 364.38M  16.5MB/s    eta 57s    \rwikipedia.csv.1      38%[======>             ] 367.43M  16.4MB/s    eta 57s    \rwikipedia.csv.1      38%[======>             ] 371.26M  16.6MB/s    eta 57s    \rwikipedia.csv.1      39%[======>             ] 374.55M  16.7MB/s    eta 57s    \rwikipedia.csv.1      39%[======>             ] 378.20M  16.7MB/s    eta 54s    \rwikipedia.csv.1      39%[======>             ] 381.66M  16.8MB/s    eta 54s    \rwikipedia.csv.1      40%[=======>            ] 385.18M  16.8MB/s    eta 54s    \rwikipedia.csv.1      40%[=======>            ] 388.87M  17.0MB/s    eta 54s    \rwikipedia.csv.1      40%[=======>            ] 392.32M  17.1MB/s    eta 54s    \rwikipedia.csv.1      41%[=======>            ] 396.01M  17.1MB/s    eta 51s    \rwikipedia.csv.1      41%[=======>            ] 399.62M  17.2MB/s    eta 51s    \rwikipedia.csv.1      42%[=======>            ] 403.17M  17.2MB/s    eta 51s    \rwikipedia.csv.1      42%[=======>            ] 406.76M  17.3MB/s    eta 51s    \rwikipedia.csv.1      42%[=======>            ] 410.51M  17.5MB/s    eta 51s    \rwikipedia.csv.1      43%[=======>            ] 413.93M  17.5MB/s    eta 49s    \rwikipedia.csv.1      43%[=======>            ] 417.67M  17.6MB/s    eta 49s    \rwikipedia.csv.1      44%[=======>            ] 421.12M  17.6MB/s    eta 49s    \rwikipedia.csv.1      44%[=======>            ] 424.88M  17.6MB/s    eta 49s    \rwikipedia.csv.1      44%[=======>            ] 428.32M  17.7MB/s    eta 49s    \rwikipedia.csv.1      45%[========>           ] 432.09M  17.9MB/s    eta 47s    \rwikipedia.csv.1      45%[========>           ] 435.49M  17.7MB/s    eta 47s    \rwikipedia.csv.1      45%[========>           ] 439.31M  17.9MB/s    eta 47s    \rwikipedia.csv.1      46%[========>           ] 442.70M  17.8MB/s    eta 47s    \rwikipedia.csv.1      46%[========>           ] 446.59M  17.9MB/s    eta 47s    \rwikipedia.csv.1      47%[========>           ] 449.90M  17.9MB/s    eta 44s    \rwikipedia.csv.1      47%[========>           ] 453.77M  18.0MB/s    eta 44s    \rwikipedia.csv.1      47%[========>           ] 457.07M  17.8MB/s    eta 44s    \rwikipedia.csv.1      48%[========>           ] 461.17M  17.9MB/s    eta 44s    \rwikipedia.csv.1      48%[========>           ] 464.38M  17.9MB/s    eta 44s    \rwikipedia.csv.1      48%[========>           ] 468.37M  18.0MB/s    eta 42s    \rwikipedia.csv.1      49%[========>           ] 471.57M  17.9MB/s    eta 42s    \rwikipedia.csv.1      49%[========>           ] 475.62M  18.0MB/s    eta 42s    \rwikipedia.csv.1      50%[=========>          ] 478.84M  17.9MB/s    eta 42s    \rwikipedia.csv.1      50%[=========>          ] 482.81M  18.0MB/s    eta 42s    \rwikipedia.csv.1      50%[=========>          ] 486.04M  17.9MB/s    eta 40s    \rwikipedia.csv.1      51%[=========>          ] 490.02M  18.1MB/s    eta 40s    \rwikipedia.csv.1      51%[=========>          ] 493.43M  17.9MB/s    eta 40s    \rwikipedia.csv.1      51%[=========>          ] 497.21M  18.0MB/s    eta 40s    \rwikipedia.csv.1      52%[=========>          ] 500.84M  17.9MB/s    eta 40s    \rwikipedia.csv.1      52%[=========>          ] 504.45M  18.0MB/s    eta 38s    \rwikipedia.csv.1      53%[=========>          ] 508.10M  18.1MB/s    eta 38s    \rwikipedia.csv.1      53%[=========>          ] 511.67M  18.0MB/s    eta 38s    \rwikipedia.csv.1      53%[=========>          ] 515.29M  18.0MB/s    eta 38s    \rwikipedia.csv.1      54%[=========>          ] 518.90M  17.9MB/s    eta 38s    \rwikipedia.csv.1      54%[=========>          ] 522.52M  18.0MB/s    eta 36s    \rwikipedia.csv.1      54%[=========>          ] 526.10M  18.0MB/s    eta 36s    \rwikipedia.csv.1      55%[==========>         ] 529.70M  17.9MB/s    eta 36s    \rwikipedia.csv.1      55%[==========>         ] 533.59M  17.9MB/s    eta 36s    \rwikipedia.csv.1      56%[==========>         ] 536.90M  17.8MB/s    eta 36s    \rwikipedia.csv.1      56%[==========>         ] 540.80M  17.9MB/s    eta 34s    \rwikipedia.csv.1      56%[==========>         ] 544.21M  17.9MB/s    eta 34s    \rwikipedia.csv.1      57%[==========>         ] 548.13M  18.1MB/s    eta 34s    \rwikipedia.csv.1      57%[==========>         ] 551.68M  18.0MB/s    eta 34s    \rwikipedia.csv.1      58%[==========>         ] 555.60M  18.0MB/s    eta 34s    \rwikipedia.csv.1      58%[==========>         ] 559.34M  18.1MB/s    eta 32s    \rwikipedia.csv.1      58%[==========>         ] 563.07M  18.1MB/s    eta 32s    \rwikipedia.csv.1      59%[==========>         ] 567.09M  18.3MB/s    eta 32s    \rwikipedia.csv.1      59%[==========>         ] 570.67M  18.3MB/s    eta 32s    \rwikipedia.csv.1      60%[===========>        ] 574.76M  18.4MB/s    eta 32s    \rwikipedia.csv.1      60%[===========>        ] 578.24M  18.3MB/s    eta 30s    \rwikipedia.csv.1      60%[===========>        ] 582.38M  18.5MB/s    eta 30s    \rwikipedia.csv.1      61%[===========>        ] 585.84M  18.5MB/s    eta 30s    \rwikipedia.csv.1      61%[===========>        ] 589.74M  18.6MB/s    eta 30s    \rwikipedia.csv.1      62%[===========>        ] 593.45M  18.7MB/s    eta 30s    \rwikipedia.csv.1      62%[===========>        ] 597.34M  18.8MB/s    eta 29s    \rwikipedia.csv.1      62%[===========>        ] 601.20M  18.7MB/s    eta 29s    \rwikipedia.csv.1      63%[===========>        ] 604.99M  18.8MB/s    eta 29s    \rwikipedia.csv.1      63%[===========>        ] 608.82M  18.9MB/s    eta 29s    \rwikipedia.csv.1      64%[===========>        ] 612.64M  18.9MB/s    eta 29s    \rwikipedia.csv.1      64%[===========>        ] 616.35M  19.0MB/s    eta 27s    \rwikipedia.csv.1      64%[===========>        ] 620.46M  19.0MB/s    eta 27s    \rwikipedia.csv.1      65%[============>       ] 624.35M  19.0MB/s    eta 27s    \rwikipedia.csv.1      65%[============>       ] 628.23M  19.1MB/s    eta 27s    \rwikipedia.csv.1      66%[============>       ] 632.04M  19.1MB/s    eta 27s    \rwikipedia.csv.1      66%[============>       ] 636.13M  19.3MB/s    eta 25s    \rwikipedia.csv.1      66%[============>       ] 639.95M  19.1MB/s    eta 25s    \rwikipedia.csv.1      67%[============>       ] 643.98M  19.2MB/s    eta 25s    \rwikipedia.csv.1      67%[============>       ] 647.79M  19.2MB/s    eta 25s    \rwikipedia.csv.1      68%[============>       ] 651.90M  19.2MB/s    eta 25s    \rwikipedia.csv.1      68%[============>       ] 655.67M  19.1MB/s    eta 23s    \rwikipedia.csv.1      68%[============>       ] 660.18M  19.4MB/s    eta 23s    \rwikipedia.csv.1      69%[============>       ] 663.67M  19.2MB/s    eta 23s    \rwikipedia.csv.1      69%[============>       ] 668.18M  19.4MB/s    eta 23s    \rwikipedia.csv.1      70%[=============>      ] 671.92M  19.4MB/s    eta 23s    \rwikipedia.csv.1      70%[=============>      ] 676.23M  19.6MB/s    eta 21s    \rwikipedia.csv.1      71%[=============>      ] 680.15M  19.7MB/s    eta 21s    \rwikipedia.csv.1      71%[=============>      ] 684.24M  19.7MB/s    eta 21s    \rwikipedia.csv.1      71%[=============>      ] 687.86M  19.5MB/s    eta 21s    \rwikipedia.csv.1      72%[=============>      ] 692.32M  19.7MB/s    eta 21s    \rwikipedia.csv.1      72%[=============>      ] 695.96M  19.5MB/s    eta 20s    \rwikipedia.csv.1      73%[=============>      ] 700.59M  19.6MB/s    eta 20s    \rwikipedia.csv.1      73%[=============>      ] 704.15M  19.5MB/s    eta 20s    \rwikipedia.csv.1      74%[=============>      ] 708.56M  19.7MB/s    eta 20s    \rwikipedia.csv.1      74%[=============>      ] 712.42M  19.7MB/s    eta 20s    \rwikipedia.csv.1      74%[=============>      ] 716.82M  19.9MB/s    eta 18s    \rwikipedia.csv.1      75%[==============>     ] 720.48M  19.7MB/s    eta 18s    \rwikipedia.csv.1      75%[==============>     ] 722.74M  19.2MB/s    eta 18s    \rwikipedia.csv.1      75%[==============>     ] 725.57M  18.7MB/s    eta 18s    \rwikipedia.csv.1      76%[==============>     ] 729.10M  18.4MB/s    eta 18s    \rwikipedia.csv.1      76%[==============>     ] 732.54M  18.4MB/s    eta 17s    \rwikipedia.csv.1      76%[==============>     ] 736.24M  18.1MB/s    eta 17s    \rwikipedia.csv.1      77%[==============>     ] 740.01M  17.7MB/s    eta 17s    \rwikipedia.csv.1      77%[==============>     ] 743.29M  17.5MB/s    eta 17s    \rwikipedia.csv.1      78%[==============>     ] 747.56M  17.7MB/s    eta 17s    \rwikipedia.csv.1      78%[==============>     ] 750.60M  17.5MB/s    eta 15s    \rwikipedia.csv.1      78%[==============>     ] 753.79M  17.1MB/s    eta 15s    \rwikipedia.csv.1      79%[==============>     ] 757.51M  17.1MB/s    eta 15s    \rwikipedia.csv.1      79%[==============>     ] 760.72M  16.9MB/s    eta 15s    \rwikipedia.csv.1      79%[==============>     ] 764.23M  16.7MB/s    eta 15s    \rwikipedia.csv.1      80%[===============>    ] 767.69M  16.5MB/s    eta 14s    \rwikipedia.csv.1      80%[===============>    ] 772.23M  16.6MB/s    eta 14s    \rwikipedia.csv.1      81%[===============>    ] 776.13M  16.6MB/s    eta 14s    \rwikipedia.csv.1      81%[===============>    ] 780.57M  17.0MB/s    eta 14s    \rwikipedia.csv.1      82%[===============>    ] 785.26M  17.5MB/s    eta 14s    \rwikipedia.csv.1      82%[===============>    ] 789.21M  17.4MB/s    eta 12s    \rwikipedia.csv.1      82%[===============>    ] 793.32M  18.1MB/s    eta 12s    \rwikipedia.csv.1      83%[===============>    ] 797.22M  18.2MB/s    eta 12s    \rwikipedia.csv.1      83%[===============>    ] 801.40M  18.4MB/s    eta 12s    \rwikipedia.csv.1      84%[===============>    ] 804.99M  18.5MB/s    eta 12s    \rwikipedia.csv.1      84%[===============>    ] 809.45M  18.8MB/s    eta 11s    \rwikipedia.csv.1      85%[================>   ] 814.09M  19.2MB/s    eta 11s    \rwikipedia.csv.1      85%[================>   ] 817.73M  19.3MB/s    eta 11s    \rwikipedia.csv.1      85%[================>   ] 822.24M  19.5MB/s    eta 11s    \rwikipedia.csv.1      86%[================>   ] 826.02M  19.6MB/s    eta 11s    \rwikipedia.csv.1      86%[================>   ] 830.15M  20.0MB/s    eta 9s     \rwikipedia.csv.1      87%[================>   ] 834.15M  19.8MB/s    eta 9s     \rwikipedia.csv.1      87%[================>   ] 836.37M  19.1MB/s    eta 9s     \rwikipedia.csv.1      87%[================>   ] 840.27M  19.3MB/s    eta 9s     \rwikipedia.csv.1      88%[================>   ] 843.51M  18.9MB/s    eta 9s     \rwikipedia.csv.1      88%[================>   ] 848.06M  19.0MB/s    eta 8s     \rwikipedia.csv.1      89%[================>   ] 852.21M  18.9MB/s    eta 8s     \rwikipedia.csv.1      89%[================>   ] 856.02M  18.9MB/s    eta 8s     \rwikipedia.csv.1      89%[================>   ] 859.66M  18.9MB/s    eta 8s     \rwikipedia.csv.1      90%[=================>  ] 863.67M  18.5MB/s    eta 8s     \rwikipedia.csv.1      90%[=================>  ] 868.32M  18.5MB/s    eta 6s     \rwikipedia.csv.1      91%[=================>  ] 872.58M  18.7MB/s    eta 6s     \rwikipedia.csv.1      91%[=================>  ] 876.67M  18.8MB/s    eta 6s     \rwikipedia.csv.1      92%[=================>  ] 880.45M  18.7MB/s    eta 6s     \rwikipedia.csv.1      92%[=================>  ] 884.62M  18.6MB/s    eta 6s     \rwikipedia.csv.1      92%[=================>  ] 889.82M  18.7MB/s    eta 5s     \rwikipedia.csv.1      93%[=================>  ] 893.76M  18.5MB/s    eta 5s     \rwikipedia.csv.1      93%[=================>  ] 897.96M  19.0MB/s    eta 5s     \rwikipedia.csv.1      94%[=================>  ] 902.25M  19.2MB/s    eta 5s     \rwikipedia.csv.1      94%[=================>  ] 905.42M  19.0MB/s    eta 5s     \rwikipedia.csv.1      95%[==================> ] 910.43M  19.2MB/s    eta 3s     \rwikipedia.csv.1      95%[==================> ] 915.07M  19.4MB/s    eta 3s     \rwikipedia.csv.1      95%[==================> ] 918.67M  19.3MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 923.12M  19.5MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 926.68M  19.6MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 931.24M  19.6MB/s    eta 2s     \rwikipedia.csv.1      97%[==================> ] 935.09M  19.5MB/s    eta 2s     \rwikipedia.csv.1      98%[==================> ] 938.30M  19.2MB/s    eta 2s     \rwikipedia.csv.1      98%[==================> ] 942.60M  19.5MB/s    eta 2s     \rwikipedia.csv.1      98%[==================> ] 946.37M  19.2MB/s    eta 2s     \rwikipedia.csv.1      99%[==================> ] 950.24M  19.0MB/s    eta 0s     \rwikipedia.csv.1      99%[==================> ] 953.48M  18.8MB/s    eta 0s     \rwikipedia.csv.1     100%[===================>] 956.99M  19.0MB/s    in 67s     \r\n\r\n2024-08-10 12:39:14 (14.4 MB/s) - ‘wikipedia.csv.1’ saved [1003477941/1003477941]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b0a872-fe88-4bc8-aa14-310761cc6d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ci predisponiamo il Dataframe spark di lavoro seguendo il seguente codice che va a leggere da un cluster AWS S3 il file csv e lo importa in un dataframe spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127e9378-64ce-496a-b5b3-a67b7c041575",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "N.b:\n",
    "Nel codice che segue, eseguiremo un campionamento stratificato del nostro dataset. Questo approccio è necessario  poiché in fase di sviluppo stiamo lavorando in un ambiente Databricks Community, pertanto avremo accesso a risorse computazionali limitate.\n",
    "\n",
    "**Processo di campionamento**\n",
    "\n",
    "1. Estrarremo un campione bilanciato del dataset originale.\n",
    "2. Utilizzeremo un metodo di campionamento stratificato basato sulla colonna \"categoria\".\n",
    "3. Questo significa che la distribuzione delle categorie nel nostro campione sarà proporzionalmente la stessa del dataset originale.\n",
    "\n",
    "Questo approccio ci permetterà di lavorare con un dataset più piccolo e gestibile, mantenendo allo stesso tempo la rappresentatività dei nostri dati originali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182dad7f-e652-4268-8a3a-945f654fa5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('/databricks/driver/wikipedia.csv')\n",
    "categoria_col = 'categoria'  \n",
    "\n",
    "# Riduciamo la size per agevolare il running nella versione community di databricks e lo facciamo con campionamento stratificato (in modo che si mantiene bilanciato)\n",
    "dataset_sample, _ = train_test_split(dataset, test_size=0.01, stratify=dataset[categoria_col], random_state=42)\n",
    "\n",
    "spark_df_ = spark.createDataFrame(dataset_sample)\n",
    "spark_df_ = spark_df_.drop(\"Unnamed: 0\")\n",
    "spark_df_.write.mode(\"overwrite\").saveAsTable(\"wikipedia\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbd1eb8-422e-4b03-9c3e-87221253909c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mostriamo il contenuto del dataframe spark di lavoro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d4765-a347-4df9-a149-03448a7f3f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+\n|               title|             summary|           documents|  categoria|\n+--------------------+--------------------+--------------------+-----------+\n|         york street|york street, also...|york street, also...|       pets|\n|             sedbury|sedbury is a vill...|sedbury is a vill...|engineering|\n|      john johnston |john johnston (17...|john johnston (17...|      trade|\n|crossboundary energy|crossboundary ene...|crossboundary ene...|     energy|\n|panki thermal pow...|panki thermal pow...|panki thermal pow...|     energy|\n|            rappbode|the rappbode is a...|the rappbode is a...|engineering|\n|1955 u.s. nationa...|first-seeded dori...|first-seeded dori...|     sports|\n|      pauline hanson|pauline lee hanso...|pauline lee hanso...|   politics|\n|boronia railway s...|boronia railway s...|boronia railway s...|  transport|\n|zetes power stations|the zonguldak ere...|the zonguldak ere...|     energy|\n|    malcolm roberts |malcolm ieuan rob...|malcolm ieuan rob...|   politics|\n| polat power station|polat power stati...|polat power stati...|     energy|\n|zetes power stations|the zonguldak ere...|the zonguldak ere...|     energy|\n|     william macewen|sir william macew...|sir william macew...|   research|\n|             jsc dux|open joint stock ...|open joint stock ...|    science|\n|              aglet |aglet is a 2020 a...|aglet is a 2020 a...| technology|\n|      westland scout|the westland scou...|the westland scou...|    science|\n|            spacecub|spacecub was a de...|spacecub was a de...|    science|\n|       pacific blue |pacific blue (for...|pacific blue (for...|     energy|\n|kammwamba thermal...|the khammwamba th...|the khammwamba th...|     energy|\n+--------------------+--------------------+--------------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark_df_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558c0d2c-5ab4-4d23-9e2f-456675c27097",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### a. Modello per campo \"Summary\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a51d85-a6dc-4ae7-8c28-fad5ddf523f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Implementeremo adesso una pipeline di Data Cleaning utilizzando SparkNLP. Il processo include:\n",
    "\n",
    "1. Normalizzazione del testo\n",
    "2. Lemmatizzazione\n",
    "3. Rimozione delle stopwords\n",
    "\n",
    "**Applicazione e Risultati**\n",
    "\n",
    "- DataFrame \"df_cleaned_summary\": Versione pulita della colonna \"summary\" su nuova colonna \"cleaned_text\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367eb18e-2e4c-446f-b989-b62ccb97723b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[OK!]\nstopwords_en download started this may take some time.\nApproximate size to download 2.9 KB\n\r[ | ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Per estrarre il testo pulito in una nuova colonna:\n",
    "from pyspark.sql.functions import concat_ws\n",
    "import re\n",
    "\n",
    "\n",
    "# Definiamo la pipeline di Spark NLP che pulisce il testo che agisce su \"summary\".\n",
    "\n",
    "# Crea un DocumentAssembler\n",
    "# Questo componente prende il testo grezzo e lo converte in un documento annotato con metadati di riferimento\n",
    "# setInputCol(\"text\"): specifica la colonna di input contenente il testo grezzo\n",
    "# setOutputCol(\"document\"): specifica la colonna di output per il documento annotato\n",
    "document_assembler = DocumentAssembler().setInputCol(\"summary\").setOutputCol(\"document\")\n",
    "\n",
    "# Crea un Tokenizer\n",
    "# Questo componente divide il testo in singole parole o token\n",
    "# setInputCols([\"document\"]): specifica la colonna di input (il documento annotato)\n",
    "# setOutputCol(\"token\"): specifica la colonna di output per i token\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# Crea un Normalizer\n",
    "# Questo componente normalizza il testo, ad esempio convertendolo in minuscolo\n",
    "# setInputCols([\"token\"]): specifica la colonna di input (i token)\n",
    "# setOutputCol(\"normalized\"): specifica la colonna di output per i token normalizzati\n",
    "# setLowercase(True): imposta la conversione in minuscolo\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setCleanupPatterns([\"[^\\w\\s]\"]) \\\n",
    "    .setLowercase(True) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# Crea un LemmatizerModel\n",
    "# Questo componente riduce le parole alla loro forma base (lemma)\n",
    "# pretrained(): carica un modello pre-addestrato per la lemmatizzazione\n",
    "# setInputCols([\"normalized\"]): specifica la colonna di input (i token normalizzati)\n",
    "# setOutputCol(\"lemma\"): specifica la colonna di output per i lemmi\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"normalized\"]).setOutputCol(\"lemma\")\n",
    "\n",
    "# Crea uno StopWordsCleaner\n",
    "# Questo componente rimuove le parole comuni (stopwords) che spesso non portano significato\n",
    "# pretrained(): carica una lista pre-definita di stopwords\n",
    "# setInputCols([\"lemma\"]): specifica la colonna di input (i lemmi)\n",
    "# setOutputCol(\"cleaned\"): specifica la colonna di output per le parole pulite\n",
    "stopwords_cleaner = StopWordsCleaner.pretrained().setInputCols([\"lemma\"]).setOutputCol(\"cleaned\")\n",
    "\n",
    "# Crea il pipeline\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer, \n",
    "    normalizer, \n",
    "    lemmatizer, \n",
    "    stopwords_cleaner\n",
    "])\n",
    "\n",
    "# Funzione per applicare la pipeline e pulire il testo\n",
    "def clean_text_spark_nlp(df, input_col=\"summary\", output_col=\"cleaned_text\"):\n",
    "    # Fit della pipeline \n",
    "    fitted_pipeline = nlp_pipeline.fit(df)\n",
    "    \n",
    "    # Applica il pipeline al DataFrame\n",
    "    result = fitted_pipeline.transform(df)\n",
    "\n",
    "    # Estraiamo il testo pulito in una nuova colonna\n",
    "    result_with_clean_text = result.withColumn(output_col, concat_ws(\" \", \"cleaned.result\"))\n",
    "\n",
    "    # Restituiamo il dataframe con la selezione delle colonne originali e la nuova colonna di testo pulito\n",
    "    return result_with_clean_text.select(df.columns + [output_col])\n",
    "\n",
    "# Applichiamo la funzione al tuo DataFrame\n",
    "df_cleaned_summary = clean_text_spark_nlp(spark_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8437175f-2ccb-4305-84ff-3ffd1f484623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mostriamo adesso la struttura del df \"df_cleaned_summary\" con la colonna aggiunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d649c5a-07dc-44f5-b074-491e025de98f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n|               title|             summary|           documents|  categoria|        cleaned_text|\n+--------------------+--------------------+--------------------+-----------+--------------------+\n|         york street|york street, also...|york street, also...|       pets|york street jakem...|\n|             sedbury|sedbury is a vill...|sedbury is a vill...|engineering|sedbury village f...|\n|      john johnston |john johnston (17...|john johnston (17...|      trade|john johnston 176...|\n|crossboundary energy|crossboundary ene...|crossboundary ene...|     energy|crossboundary ene...|\n|panki thermal pow...|panki thermal pow...|panki thermal pow...|     energy|panki thermal pow...|\n|            rappbode|the rappbode is a...|the rappbode is a...|engineering|rappbode righthan...|\n|1955 u.s. nationa...|first-seeded dori...|first-seeded dori...|     sports|firstseeded doris...|\n|      pauline hanson|pauline lee hanso...|pauline lee hanso...|   politics|pauline lee hanso...|\n|boronia railway s...|boronia railway s...|boronia railway s...|  transport|boronia railway s...|\n|zetes power stations|the zonguldak ere...|the zonguldak ere...|     energy|zonguldak eren te...|\n|    malcolm roberts |malcolm ieuan rob...|malcolm ieuan rob...|   politics|malcolm ieuan rob...|\n| polat power station|polat power stati...|polat power stati...|     energy|polat power stati...|\n|zetes power stations|the zonguldak ere...|the zonguldak ere...|     energy|zonguldak eren te...|\n|     william macewen|sir william macew...|sir william macew...|   research|sir william macew...|\n|             jsc dux|open joint stock ...|open joint stock ...|    science|open joint stock ...|\n|              aglet |aglet is a 2020 a...|aglet is a 2020 a...| technology|aglet 2020 augmen...|\n|      westland scout|the westland scou...|the westland scou...|    science|westland scout li...|\n|            spacecub|spacecub was a de...|spacecub was a de...|    science|spacecub design p...|\n|       pacific blue |pacific blue (for...|pacific blue (for...|     energy|pacific blue paci...|\n|kammwamba thermal...|the khammwamba th...|the khammwamba th...|     energy|khammwamba therma...|\n+--------------------+--------------------+--------------------+-----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559d8e6d-fd19-49df-a647-2fc93e30711d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Quello che ora dobbiamo fare sarà quello di definire la vettorizzazione tramite HashingTF più efficace del CountVectorizer in quanto mappa direttamente le parole in indici del vettore delle feature usando una funzione di hash. Questo riduce il tempo e la memoria necessari per costruire un vocabolario esplicito, come invece avviene con l'ausilio del  CountVectorizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c4f731-23cd-428a-a5eb-fe28c6e0dedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.sql.functions import udf, sum as spark_sum, split\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Tokenizziamo la colonna \"cleaned_text\" \n",
    "df_cleaned_summary = df_cleaned_summary.withColumn(\"words\", split(df_cleaned_summary.cleaned_text, \"\\\\s+\"))\n",
    "\n",
    "\n",
    "# Definiamo HashingTF sulla colonna \"words\"\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"word_vector\") \n",
    "\n",
    "# Effettuo la Caching dei dati per ridurre i tempi di lettura\n",
    "df_cleaned_summary.cache()\n",
    "\n",
    "# Applico la trasformazione\n",
    "df_cleaned_summary_cont_vect = hashingTF.transform(df_cleaned_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979ced87-b0bd-4cec-8926-b41b4da95069",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Andiamo a vedere il tipo di colonna che mi ha generato nel dataframe che abbiamo denominato \"word_vector\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0c691c-f82d-47d0-968b-c11eca772e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- title: string (nullable = true)\n |-- summary: string (nullable = true)\n |-- documents: string (nullable = true)\n |-- categoria: string (nullable = true)\n |-- cleaned_text: string (nullable = false)\n |-- words: array (nullable = false)\n |    |-- element: string (containsNull = false)\n |-- word_vector: vector (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_summary_cont_vect.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab275634-846c-4ac8-bc7b-9f3e28034ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Abbiamo ottenuto una nuova colonna chiamata \"word_vector\". Questa colonna contiene vettori sparsi che rappresentano il conteggio delle parole per ogni documento. Ecco come procederemo per analizzare questi dati:\n",
    "\n",
    "1. Comprensione della Colonna \"word_vector\"\n",
    "- La colonna \"word_vector\" è di tipo \"vector\".\n",
    "- Ogni elemento di questo vettore rappresenta una parola unica nel vocabolario.\n",
    "- I valori diversi da zero in questo vettore indicano la presenza e la frequenza di una parola nel documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb6603c-f5ce-4537-a613-a3d3a063596e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Procedo a crearmi il Classificatore ed addestrarlo per la colonna \"summary\" e mostriamo le relative performance. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07a4d90c-f60d-4e99-902e-b81cb6bb25da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Iniziamo a rendere codificabile la variabile target \"categoria\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3886062b-cda6-4403-b71b-c795bc6d1fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from itertools import chain\n",
    "\n",
    "def custom_category_indexer(df, input_col, output_col):\n",
    "    # Ottieni tutte le categorie uniche\n",
    "    categories = df.select(input_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Crea un dizionario di mapping categoria -> indice\n",
    "    category_dict = {cat: idx for idx, cat in enumerate(categories)}\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def optimized_category_indexer(df, input_col, output_col):\n",
    "    # Crea un DataFrame con categorie uniche e indici\n",
    "    category_df = df.select(input_col).distinct()\n",
    "    \n",
    "    # Usa row_number per assegnare indici univoci\n",
    "    window = Window.orderBy(monotonically_increasing_id())\n",
    "    category_df = category_df.withColumn(output_col, row_number().over(window) - 1)\n",
    "    \n",
    "    # Esegui un join per assegnare gli indici al DataFrame originale\n",
    "    df_indexed = df.join(category_df, on=input_col, how=\"left\")\n",
    "    \n",
    "    return df_indexed.withColumn(output_col, col(output_col).cast(IntegerType()))\n",
    "\n",
    "# Uso della funzione\n",
    "df_cleaned_summary_cont_vect = optimized_category_indexer(df_cleaned_summary_cont_vect, \"categoria\", \"categoriaIndex\")\n",
    "\n",
    "# Dividiamo il dataset in training e test\n",
    "train, test = df_cleaned_summary_cont_vect.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Cache dei dati per migliorare le prestazioni\n",
    "train.cache()\n",
    "test.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b29b692-915f-46b3-81f9-1797c6a1bd24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Procediamo adesso all'implementazione ed addestramento del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6817d65b-ea24-480b-978b-7b1dd07b5f98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8830082319255548\n+----------+----------------------+----------+--------------------+\n| categoria|categoriaIndex_summary|prediction|         probability|\n+----------+----------------------+----------+--------------------+\n|technology|                   8.0|       8.0|[7.58220915235418...|\n|    sports|                  14.0|      14.0|[2.02391659606794...|\n|    sports|                  14.0|      14.0|[1.32120731570598...|\n|    sports|                  14.0|      14.0|[8.40768403813396...|\n|    sports|                  14.0|      14.0|[1.01821115752587...|\n|    sports|                  14.0|      14.0|[9.87376050369236...|\n|    sports|                  14.0|      14.0|[1.20523668366701...|\n|    sports|                  14.0|      14.0|[4.89522599126926...|\n|    sports|                  14.0|      14.0|[7.60023971594721...|\n|    sports|                  14.0|      14.0|[2.48079553034763...|\n+----------+----------------------+----------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Modello \n",
    "lr = LogisticRegression(featuresCol=\"word_vector\", labelCol=\"categoriaIndex\", maxIter=3)\n",
    "model_documents = lr.fit(train)\n",
    "\n",
    "# Predizioni\n",
    "predictions_documents = model_documents.transform(train)\n",
    "\n",
    "\n",
    "# Valutiamo il modello con la colonna corretta per la classificazione\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoriaIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_documents)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Mostriamo alcune predizioni, usando le colonne corrette\n",
    "predictions_documents.select(\"categoria\", \"categoriaIndex\", \"prediction\", \"probability\").show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Progetto_Classificatore (campo Summary)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
