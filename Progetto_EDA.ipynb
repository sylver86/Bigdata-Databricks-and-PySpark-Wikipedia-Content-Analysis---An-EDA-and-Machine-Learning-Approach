{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb815bf-3666-4ee9-974f-5fe911a9f12c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Attività di EDA su Wikipedia\n",
    "\n",
    "## Descrizione del dataset\n",
    "\n",
    "Il dataset offerto è composto da 4 colonne:\n",
    "\n",
    "- **title**: indica il titolo dell'articolo\n",
    "- **summary**: contiene l'introduzione dell'articolo\n",
    "- **documents**: contiene l'articolo completo\n",
    "- **categoria**: contiene la categoria associata all'articolo\n",
    "\n",
    "## Obiettivi\n",
    "\n",
    "### 1. Attività EDA:\n",
    "È necessario svolgere un'attività di EDA per analizzare e valutare statisticamente tutto il contenuto informativo offerto da Wikipedia. Il dataset fornito possiede le seguenti categorie:\n",
    "\n",
    "- 'culture'\n",
    "- 'economics'\n",
    "- 'energy'\n",
    "- 'engineering'\n",
    "- 'finance'\n",
    "- 'humanities'\n",
    "- 'medicine'\n",
    "- 'pets'\n",
    "- 'politics'\n",
    "- 'research'\n",
    "- 'science'\n",
    "- 'sports'\n",
    "- 'technology'\n",
    "- 'trade'\n",
    "- 'transport'\n",
    "\n",
    "Per ogni categoria, calcolare le seguenti informazioni:\n",
    "\n",
    "1. Numero di articoli\n",
    "2. Numero medio di parole utilizzate\n",
    "3. Numero massimo di parole presenti nell'articolo più lungo\n",
    "4. Numero minimo di parole presenti nell'articolo più corto\n",
    "5. Per ogni categoria, individuare la nuvola di parole più rappresentativa\n",
    "\n",
    "\n",
    "### 2. Sviluppo classificatore NLP articoli :\n",
    "\n",
    "Dopo aver svolto l'analisi richiesta, addestrare e testare un classificatore testuale capace di classificare gli articoli (secondo le categorie presenti nel dataset) che saranno in futuro inseriti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce43c30-06db-4b7f-9d88-e3a4c321b8ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Attività EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25c79e2-6f23-415d-8d0c-c3825186c2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Iniziamo il progetto andandoci a scaricare il dataset di lavoro :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4ac5b6-af00-4772-af88-a11f8856128d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-10 14:46:02--  https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\r\nResolving proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)... 3.5.225.173, 52.95.156.56\r\nConnecting to proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)|3.5.225.173|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1003477941 (957M) [text/csv]\r\nSaving to: ‘wikipedia.csv.1’\r\n\r\n\rwikipedia.csv.1       0%[                    ]       0  --.-KB/s               \rwikipedia.csv.1       0%[                    ]  49.49K   185KB/s               \rwikipedia.csv.1       0%[                    ] 108.49K   202KB/s               \rwikipedia.csv.1       0%[                    ] 162.54K   202KB/s               \rwikipedia.csv.1       0%[                    ] 233.06K   217KB/s               \rwikipedia.csv.1       0%[                    ] 315.11K   235KB/s               \rwikipedia.csv.1       0%[                    ] 393.63K   245KB/s               \rwikipedia.csv.1       0%[                    ] 472.73K   252KB/s               \rwikipedia.csv.1       0%[                    ] 578.78K   270KB/s               \rwikipedia.csv.1       0%[                    ] 681.09K   282KB/s               \rwikipedia.csv.1       0%[                    ] 763.35K   284KB/s               \rwikipedia.csv.1       0%[                    ] 865.35K   292KB/s               \rwikipedia.csv.1       0%[                    ] 984.35K   305KB/s    eta 53m 30s\rwikipedia.csv.1       0%[                    ]   1.09M   320KB/s    eta 53m 30s\rwikipedia.csv.1       0%[                    ]   1.24M   338KB/s    eta 53m 30s\rwikipedia.csv.1       0%[                    ]   1.41M   358KB/s    eta 53m 30s\rwikipedia.csv.1       0%[                    ]   1.58M   376KB/s    eta 43m 20s\rwikipedia.csv.1       0%[                    ]   1.77M   398KB/s    eta 43m 20s\rwikipedia.csv.1       0%[                    ]   1.99M   437KB/s    eta 43m 20s\rwikipedia.csv.1       0%[                    ]   2.24M   495KB/s    eta 43m 20s\rwikipedia.csv.1       0%[                    ]   2.50M   542KB/s    eta 34m 9s \rwikipedia.csv.1       0%[                    ]   2.79M   590KB/s    eta 34m 9s \rwikipedia.csv.1       0%[                    ]   3.10M   671KB/s    eta 34m 9s \rwikipedia.csv.1       0%[                    ]   3.46M   724KB/s    eta 34m 9s \rwikipedia.csv.1       0%[                    ]   3.85M   795KB/s    eta 26m 35s\rwikipedia.csv.1       0%[                    ]   4.28M   900KB/s    eta 26m 35s\rwikipedia.csv.1       0%[                    ]   4.75M   980KB/s    eta 26m 35s\rwikipedia.csv.1       0%[                    ]   5.28M  1.05MB/s    eta 26m 35s\rwikipedia.csv.1       0%[                    ]   5.84M  1.18MB/s    eta 20m 24s\rwikipedia.csv.1       0%[                    ]   6.47M  1.28MB/s    eta 20m 24s\rwikipedia.csv.1       0%[                    ]   7.18M  1.41MB/s    eta 20m 24s\rwikipedia.csv.1       0%[                    ]   7.95M  1.58MB/s    eta 20m 24s\rwikipedia.csv.1       0%[                    ]   8.80M  1.72MB/s    eta 15m 26s\rwikipedia.csv.1       1%[                    ]   9.73M  1.89MB/s    eta 15m 26s\rwikipedia.csv.1       1%[                    ]  10.76M  2.12MB/s    eta 15m 26s\rwikipedia.csv.1       1%[                    ]  11.90M  2.30MB/s    eta 15m 26s\rwikipedia.csv.1       1%[                    ]  13.15M  2.53MB/s    eta 11m 33s\rwikipedia.csv.1       1%[                    ]  14.52M  2.87MB/s    eta 11m 33s\rwikipedia.csv.1       1%[                    ]  15.43M  3.01MB/s    eta 11m 33s\rwikipedia.csv.1       1%[                    ]  16.87M  3.29MB/s    eta 11m 33s\rwikipedia.csv.1       1%[                    ]  18.62M  3.61MB/s    eta 11m 33s\rwikipedia.csv.1       2%[                    ]  19.77M  3.91MB/s    eta 8m 30s \rwikipedia.csv.1       2%[                    ]  21.62M  4.27MB/s    eta 8m 30s \rwikipedia.csv.1       2%[                    ]  23.83M  4.72MB/s    eta 8m 30s \rwikipedia.csv.1       2%[                    ]  25.10M  4.95MB/s    eta 8m 30s \rwikipedia.csv.1       2%[                    ]  27.46M  5.60MB/s    eta 8m 30s \rwikipedia.csv.1       3%[                    ]  29.10M  5.87MB/s    eta 6m 16s \rwikipedia.csv.1       3%[                    ]  32.04M  6.51MB/s    eta 6m 16s \rwikipedia.csv.1       3%[                    ]  33.93M  6.79MB/s    eta 6m 16s \rwikipedia.csv.1       3%[                    ]  36.71M  7.50MB/s    eta 6m 16s \rwikipedia.csv.1       4%[                    ]  39.77M  8.13MB/s    eta 6m 16s \rwikipedia.csv.1       4%[                    ]  41.65M  8.35MB/s    eta 4m 44s \rwikipedia.csv.1       4%[                    ]  44.43M  9.03MB/s    eta 4m 44s \rwikipedia.csv.1       4%[                    ]  47.54M  9.47MB/s    eta 4m 44s \rwikipedia.csv.1       5%[>                   ]  49.44M  9.71MB/s    eta 4m 44s \rwikipedia.csv.1       5%[>                   ]  52.29M  10.1MB/s    eta 4m 44s \rwikipedia.csv.1       5%[>                   ]  55.44M  10.6MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  58.63M  11.0MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  60.57M  11.2MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  63.49M  11.3MB/s    eta 3m 49s \rwikipedia.csv.1       6%[>                   ]  66.11M  11.6MB/s    eta 3m 49s \rwikipedia.csv.1       7%[>                   ]  68.40M  11.6MB/s    eta 3m 17s \rwikipedia.csv.1       7%[>                   ]  71.71M  11.9MB/s    eta 3m 17s \rwikipedia.csv.1       7%[>                   ]  74.95M  12.0MB/s    eta 3m 17s \rwikipedia.csv.1       8%[>                   ]  76.75M  11.8MB/s    eta 3m 17s \rwikipedia.csv.1       8%[>                   ]  80.08M  12.2MB/s    eta 3m 17s \rwikipedia.csv.1       8%[>                   ]  82.07M  12.0MB/s    eta 2m 53s \rwikipedia.csv.1       8%[>                   ]  85.15M  12.1MB/s    eta 2m 53s \rwikipedia.csv.1       9%[>                   ]  87.15M  12.2MB/s    eta 2m 53s \rwikipedia.csv.1       9%[>                   ]  90.21M  12.3MB/s    eta 2m 53s \rwikipedia.csv.1       9%[>                   ]  92.22M  12.1MB/s    eta 2m 53s \rwikipedia.csv.1       9%[>                   ]  95.29M  12.5MB/s    eta 2m 36s \rwikipedia.csv.1      10%[=>                  ]  97.33M  12.4MB/s    eta 2m 36s \rwikipedia.csv.1      10%[=>                  ] 100.51M  12.5MB/s    eta 2m 36s \rwikipedia.csv.1      10%[=>                  ] 102.57M  12.3MB/s    eta 2m 36s \rwikipedia.csv.1      11%[=>                  ] 105.77M  12.8MB/s    eta 2m 36s \rwikipedia.csv.1      11%[=>                  ] 107.83M  12.7MB/s    eta 2m 24s \rwikipedia.csv.1      11%[=>                  ] 111.12M  12.6MB/s    eta 2m 24s \rwikipedia.csv.1      11%[=>                  ] 114.51M  13.0MB/s    eta 2m 24s \rwikipedia.csv.1      12%[=>                  ] 116.57M  12.9MB/s    eta 2m 24s \rwikipedia.csv.1      12%[=>                  ] 120.12M  13.0MB/s    eta 2m 24s \rwikipedia.csv.1      12%[=>                  ] 122.07M  12.8MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 125.58M  13.3MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 127.58M  13.2MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 130.83M  13.3MB/s    eta 2m 12s \rwikipedia.csv.1      13%[=>                  ] 133.07M  13.1MB/s    eta 2m 12s \rwikipedia.csv.1      14%[=>                  ] 136.47M  13.6MB/s    eta 2m 2s  \rwikipedia.csv.1      14%[=>                  ] 138.57M  13.5MB/s    eta 2m 2s  \rwikipedia.csv.1      14%[=>                  ] 142.14M  13.6MB/s    eta 2m 2s  \rwikipedia.csv.1      15%[==>                 ] 144.18M  13.4MB/s    eta 2m 2s  \rwikipedia.csv.1      15%[==>                 ] 147.53M  13.6MB/s    eta 2m 2s  \rwikipedia.csv.1      15%[==>                 ] 149.79M  13.7MB/s    eta 1m 55s \rwikipedia.csv.1      15%[==>                 ] 153.10M  13.7MB/s    eta 1m 55s \rwikipedia.csv.1      16%[==>                 ] 155.62M  13.5MB/s    eta 1m 55s \rwikipedia.csv.1      16%[==>                 ] 159.22M  14.0MB/s    eta 1m 55s \rwikipedia.csv.1      16%[==>                 ] 161.37M  13.8MB/s    eta 1m 55s \rwikipedia.csv.1      17%[==>                 ] 164.91M  13.9MB/s    eta 1m 47s \rwikipedia.csv.1      17%[==>                 ] 167.05M  13.7MB/s    eta 1m 47s \rwikipedia.csv.1      17%[==>                 ] 170.60M  14.1MB/s    eta 1m 47s \rwikipedia.csv.1      18%[==>                 ] 172.74M  14.0MB/s    eta 1m 47s \rwikipedia.csv.1      18%[==>                 ] 176.32M  14.1MB/s    eta 1m 47s \rwikipedia.csv.1      18%[==>                 ] 178.38M  13.8MB/s    eta 1m 42s \rwikipedia.csv.1      18%[==>                 ] 180.83M  13.8MB/s    eta 1m 42s \rwikipedia.csv.1      19%[==>                 ] 184.46M  14.0MB/s    eta 1m 42s \rwikipedia.csv.1      19%[==>                 ] 187.12M  13.8MB/s    eta 1m 42s \rwikipedia.csv.1      19%[==>                 ] 190.08M  13.8MB/s    eta 1m 42s \rwikipedia.csv.1      20%[===>                ] 192.83M  13.8MB/s    eta 97s    \rwikipedia.csv.1      20%[===>                ] 195.80M  14.0MB/s    eta 97s    \rwikipedia.csv.1      20%[===>                ] 198.58M  14.1MB/s    eta 97s    \rwikipedia.csv.1      21%[===>                ] 201.54M  14.0MB/s    eta 97s    \rwikipedia.csv.1      21%[===>                ] 204.43M  13.9MB/s    eta 97s    \rwikipedia.csv.1      21%[===>                ] 207.38M  14.1MB/s    eta 92s    \rwikipedia.csv.1      21%[===>                ] 210.30M  14.2MB/s    eta 92s    \rwikipedia.csv.1      22%[===>                ] 213.26M  14.0MB/s    eta 92s    \rwikipedia.csv.1      22%[===>                ] 216.18M  14.0MB/s    eta 92s    \rwikipedia.csv.1      22%[===>                ] 218.91M  14.1MB/s    eta 92s    \rwikipedia.csv.1      23%[===>                ] 221.88M  14.2MB/s    eta 88s    \rwikipedia.csv.1      23%[===>                ] 224.99M  14.1MB/s    eta 88s    \rwikipedia.csv.1      23%[===>                ] 228.16M  14.4MB/s    eta 88s    \rwikipedia.csv.1      24%[===>                ] 231.40M  14.6MB/s    eta 88s    \rwikipedia.csv.1      24%[===>                ] 234.04M  14.5MB/s    eta 88s    \rwikipedia.csv.1      24%[===>                ] 237.37M  14.5MB/s    eta 83s    \rwikipedia.csv.1      25%[====>               ] 239.91M  14.4MB/s    eta 83s    \rwikipedia.csv.1      25%[====>               ] 243.24M  14.6MB/s    eta 83s    \rwikipedia.csv.1      25%[====>               ] 245.80M  14.6MB/s    eta 83s    \rwikipedia.csv.1      26%[====>               ] 249.13M  14.5MB/s    eta 83s    \rwikipedia.csv.1      26%[====>               ] 251.93M  14.5MB/s    eta 80s    \rwikipedia.csv.1      26%[====>               ] 255.02M  14.6MB/s    eta 80s    \rwikipedia.csv.1      26%[====>               ] 257.83M  14.6MB/s    eta 80s    \rwikipedia.csv.1      27%[====>               ] 260.97M  14.6MB/s    eta 80s    \rwikipedia.csv.1      27%[====>               ] 263.77M  14.6MB/s    eta 80s    \rwikipedia.csv.1      27%[====>               ] 266.93M  14.8MB/s    eta 76s    \rwikipedia.csv.1      28%[====>               ] 269.79M  14.7MB/s    eta 76s    \rwikipedia.csv.1      28%[====>               ] 273.02M  14.8MB/s    eta 76s    \rwikipedia.csv.1      28%[====>               ] 275.91M  14.8MB/s    eta 76s    \rwikipedia.csv.1      29%[====>               ] 279.13M  14.9MB/s    eta 76s    \rwikipedia.csv.1      29%[====>               ] 282.15M  14.9MB/s    eta 73s    \rwikipedia.csv.1      29%[====>               ] 285.30M  14.9MB/s    eta 73s    \rwikipedia.csv.1      30%[=====>              ] 288.24M  14.9MB/s    eta 73s    \rwikipedia.csv.1      30%[=====>              ] 291.47M  15.1MB/s    eta 73s    \rwikipedia.csv.1      30%[=====>              ] 294.41M  15.0MB/s    eta 73s    \rwikipedia.csv.1      31%[=====>              ] 297.71M  15.1MB/s    eta 70s    \rwikipedia.csv.1      31%[=====>              ] 300.65M  15.2MB/s    eta 70s    \rwikipedia.csv.1      31%[=====>              ] 303.93M  15.2MB/s    eta 70s    \rwikipedia.csv.1      32%[=====>              ] 307.47M  15.4MB/s    eta 70s    \rwikipedia.csv.1      32%[=====>              ] 310.18M  15.3MB/s    eta 70s    \rwikipedia.csv.1      32%[=====>              ] 313.76M  15.4MB/s    eta 67s    \rwikipedia.csv.1      33%[=====>              ] 316.46M  15.3MB/s    eta 67s    \rwikipedia.csv.1      33%[=====>              ] 320.02M  15.6MB/s    eta 67s    \rwikipedia.csv.1      33%[=====>              ] 322.71M  15.3MB/s    eta 67s    \rwikipedia.csv.1      34%[=====>              ] 326.26M  15.5MB/s    eta 67s    \rwikipedia.csv.1      34%[=====>              ] 328.96M  15.4MB/s    eta 64s    \rwikipedia.csv.1      34%[=====>              ] 332.51M  15.6MB/s    eta 64s    \rwikipedia.csv.1      35%[======>             ] 335.22M  15.5MB/s    eta 64s    \rwikipedia.csv.1      35%[======>             ] 338.77M  15.5MB/s    eta 64s    \rwikipedia.csv.1      35%[======>             ] 341.47M  15.4MB/s    eta 64s    \rwikipedia.csv.1      36%[======>             ] 345.02M  15.5MB/s    eta 61s    \rwikipedia.csv.1      36%[======>             ] 347.77M  15.5MB/s    eta 61s    \rwikipedia.csv.1      36%[======>             ] 350.72M  15.1MB/s    eta 61s    \rwikipedia.csv.1      37%[======>             ] 354.71M  15.4MB/s    eta 61s    \rwikipedia.csv.1      37%[======>             ] 357.41M  15.2MB/s    eta 61s    \rwikipedia.csv.1      37%[======>             ] 361.54M  15.6MB/s    eta 59s    \rwikipedia.csv.1      38%[======>             ] 363.71M  15.2MB/s    eta 59s    \rwikipedia.csv.1      38%[======>             ] 367.55M  15.5MB/s    eta 59s    \rwikipedia.csv.1      38%[======>             ] 370.15M  15.3MB/s    eta 59s    \rwikipedia.csv.1      39%[======>             ] 373.65M  15.4MB/s    eta 59s    \rwikipedia.csv.1      39%[======>             ] 376.41M  15.4MB/s    eta 56s    \rwikipedia.csv.1      39%[======>             ] 379.94M  15.4MB/s    eta 56s    \rwikipedia.csv.1      39%[======>             ] 382.66M  15.3MB/s    eta 56s    \rwikipedia.csv.1      40%[=======>            ] 386.46M  15.4MB/s    eta 56s    \rwikipedia.csv.1      40%[=======>            ] 389.05M  15.3MB/s    eta 56s    \rwikipedia.csv.1      41%[=======>            ] 393.05M  15.7MB/s    eta 54s    \rwikipedia.csv.1      41%[=======>            ] 395.57M  15.4MB/s    eta 54s    \rwikipedia.csv.1      41%[=======>            ] 399.65M  15.7MB/s    eta 54s    \rwikipedia.csv.1      42%[=======>            ] 402.05M  15.9MB/s    eta 54s    \rwikipedia.csv.1      42%[=======>            ] 406.13M  15.9MB/s    eta 54s    \rwikipedia.csv.1      42%[=======>            ] 408.49M  15.6MB/s    eta 52s    \rwikipedia.csv.1      43%[=======>            ] 412.66M  15.8MB/s    eta 52s    \rwikipedia.csv.1      43%[=======>            ] 415.02M  15.9MB/s    eta 52s    \rwikipedia.csv.1      43%[=======>            ] 419.19M  16.1MB/s    eta 52s    \rwikipedia.csv.1      44%[=======>            ] 421.55M  15.8MB/s    eta 52s    \rwikipedia.csv.1      44%[=======>            ] 425.74M  16.1MB/s    eta 49s    \rwikipedia.csv.1      44%[=======>            ] 428.08M  16.0MB/s    eta 49s    \rwikipedia.csv.1      45%[========>           ] 432.24M  16.3MB/s    eta 49s    \rwikipedia.csv.1      45%[========>           ] 434.58M  15.9MB/s    eta 49s    \rwikipedia.csv.1      45%[========>           ] 438.77M  16.2MB/s    eta 49s    \rwikipedia.csv.1      46%[========>           ] 441.19M  15.9MB/s    eta 48s    \rwikipedia.csv.1      46%[========>           ] 445.41M  16.5MB/s    eta 48s    \rwikipedia.csv.1      46%[========>           ] 447.87M  16.3MB/s    eta 48s    \rwikipedia.csv.1      47%[========>           ] 452.08M  16.4MB/s    eta 48s    \rwikipedia.csv.1      47%[========>           ] 454.54M  16.0MB/s    eta 48s    \rwikipedia.csv.1      47%[========>           ] 458.76M  16.4MB/s    eta 45s    \rwikipedia.csv.1      48%[========>           ] 461.21M  16.4MB/s    eta 45s    \rwikipedia.csv.1      48%[========>           ] 465.37M  16.6MB/s    eta 45s    \rwikipedia.csv.1      48%[========>           ] 467.90M  16.3MB/s    eta 45s    \rwikipedia.csv.1      49%[========>           ] 472.19M  16.4MB/s    eta 45s    \rwikipedia.csv.1      49%[========>           ] 474.63M  16.2MB/s    eta 43s    \rwikipedia.csv.1      50%[=========>          ] 478.93M  16.8MB/s    eta 43s    \rwikipedia.csv.1      50%[=========>          ] 481.35M  16.5MB/s    eta 43s    \rwikipedia.csv.1      50%[=========>          ] 485.65M  16.7MB/s    eta 43s    \rwikipedia.csv.1      51%[=========>          ] 488.07M  16.3MB/s    eta 43s    \rwikipedia.csv.1      51%[=========>          ] 492.40M  16.9MB/s    eta 41s    \rwikipedia.csv.1      51%[=========>          ] 494.85M  16.7MB/s    eta 41s    \rwikipedia.csv.1      52%[=========>          ] 499.19M  16.9MB/s    eta 41s    \rwikipedia.csv.1      52%[=========>          ] 501.66M  16.4MB/s    eta 41s    \rwikipedia.csv.1      52%[=========>          ] 506.05M  16.8MB/s    eta 41s    \rwikipedia.csv.1      53%[=========>          ] 508.52M  16.7MB/s    eta 39s    \rwikipedia.csv.1      53%[=========>          ] 512.93M  17.1MB/s    eta 39s    \rwikipedia.csv.1      53%[=========>          ] 515.37M  16.7MB/s    eta 39s    \rwikipedia.csv.1      54%[=========>          ] 519.80M  16.9MB/s    eta 39s    \rwikipedia.csv.1      54%[=========>          ] 522.26M  16.9MB/s    eta 39s    \rwikipedia.csv.1      55%[==========>         ] 526.38M  17.1MB/s    eta 37s    \rwikipedia.csv.1      55%[==========>         ] 529.19M  16.8MB/s    eta 37s    \rwikipedia.csv.1      55%[==========>         ] 533.49M  17.0MB/s    eta 37s    \rwikipedia.csv.1      56%[==========>         ] 536.26M  16.9MB/s    eta 37s    \rwikipedia.csv.1      56%[==========>         ] 540.58M  17.3MB/s    eta 37s    \rwikipedia.csv.1      56%[==========>         ] 543.40M  17.2MB/s    eta 36s    \rwikipedia.csv.1      57%[==========>         ] 547.77M  17.3MB/s    eta 36s    \rwikipedia.csv.1      57%[==========>         ] 550.60M  17.2MB/s    eta 36s    \rwikipedia.csv.1      57%[==========>         ] 554.94M  17.4MB/s    eta 36s    \rwikipedia.csv.1      58%[==========>         ] 557.72M  17.4MB/s    eta 36s    \rwikipedia.csv.1      58%[==========>         ] 562.15M  17.7MB/s    eta 33s    \rwikipedia.csv.1      59%[==========>         ] 564.94M  17.4MB/s    eta 33s    \rwikipedia.csv.1      59%[==========>         ] 569.34M  17.6MB/s    eta 33s    \rwikipedia.csv.1      59%[==========>         ] 572.15M  17.4MB/s    eta 33s    \rwikipedia.csv.1      60%[===========>        ] 575.94M  17.7MB/s    eta 33s    \rwikipedia.csv.1      60%[===========>        ] 579.44M  17.8MB/s    eta 32s    \rwikipedia.csv.1      60%[===========>        ] 582.82M  17.6MB/s    eta 32s    \rwikipedia.csv.1      61%[===========>        ] 586.76M  17.6MB/s    eta 32s    \rwikipedia.csv.1      61%[===========>        ] 590.46M  17.6MB/s    eta 32s    \rwikipedia.csv.1      62%[===========>        ] 594.41M  17.9MB/s    eta 32s    \rwikipedia.csv.1      62%[===========>        ] 597.85M  17.8MB/s    eta 30s    \rwikipedia.csv.1      62%[===========>        ] 601.79M  17.7MB/s    eta 30s    \rwikipedia.csv.1      63%[===========>        ] 605.40M  17.6MB/s    eta 30s    \rwikipedia.csv.1      63%[===========>        ] 609.24M  17.9MB/s    eta 30s    \rwikipedia.csv.1      64%[===========>        ] 612.81M  18.0MB/s    eta 30s    \rwikipedia.csv.1      64%[===========>        ] 616.37M  17.9MB/s    eta 28s    \rwikipedia.csv.1      64%[===========>        ] 620.16M  17.8MB/s    eta 28s    \rwikipedia.csv.1      65%[============>       ] 623.82M  17.8MB/s    eta 28s    \rwikipedia.csv.1      65%[============>       ] 627.68M  18.0MB/s    eta 28s    \rwikipedia.csv.1      65%[============>       ] 631.32M  18.1MB/s    eta 28s    \rwikipedia.csv.1      66%[============>       ] 635.21M  18.1MB/s    eta 26s    \rwikipedia.csv.1      66%[============>       ] 638.89M  18.3MB/s    eta 26s    \rwikipedia.csv.1      67%[============>       ] 642.80M  18.4MB/s    eta 26s    \rwikipedia.csv.1      67%[============>       ] 646.47M  18.5MB/s    eta 26s    \rwikipedia.csv.1      67%[============>       ] 650.44M  18.5MB/s    eta 26s    \rwikipedia.csv.1      68%[============>       ] 654.08M  18.5MB/s    eta 24s    \rwikipedia.csv.1      68%[============>       ] 658.08M  18.6MB/s    eta 24s    \rwikipedia.csv.1      69%[============>       ] 661.71M  18.6MB/s    eta 24s    \rwikipedia.csv.1      69%[============>       ] 665.68M  18.7MB/s    eta 24s    \rwikipedia.csv.1      69%[============>       ] 669.49M  18.8MB/s    eta 24s    \rwikipedia.csv.1      70%[=============>      ] 673.57M  18.8MB/s    eta 23s    \rwikipedia.csv.1      70%[=============>      ] 677.37M  18.9MB/s    eta 23s    \rwikipedia.csv.1      71%[=============>      ] 681.51M  19.1MB/s    eta 23s    \rwikipedia.csv.1      71%[=============>      ] 685.30M  19.1MB/s    eta 23s    \rwikipedia.csv.1      72%[=============>      ] 689.49M  19.1MB/s    eta 23s    \rwikipedia.csv.1      72%[=============>      ] 693.27M  19.2MB/s    eta 21s    \rwikipedia.csv.1      72%[=============>      ] 697.15M  19.2MB/s    eta 21s    \rwikipedia.csv.1      73%[=============>      ] 701.51M  19.3MB/s    eta 21s    \rwikipedia.csv.1      73%[=============>      ] 705.21M  19.2MB/s    eta 21s    \rwikipedia.csv.1      74%[=============>      ] 709.62M  19.5MB/s    eta 21s    \rwikipedia.csv.1      74%[=============>      ] 713.88M  19.6MB/s    eta 19s    \rwikipedia.csv.1      75%[==============>     ] 717.79M  19.6MB/s    eta 19s    \rwikipedia.csv.1      75%[==============>     ] 721.97M  19.7MB/s    eta 19s    \rwikipedia.csv.1      75%[==============>     ] 725.83M  19.8MB/s    eta 19s    \rwikipedia.csv.1      76%[==============>     ] 730.07M  19.9MB/s    eta 19s    \rwikipedia.csv.1      76%[==============>     ] 733.87M  19.8MB/s    eta 17s    \rwikipedia.csv.1      77%[==============>     ] 737.92M  19.8MB/s    eta 17s    \rwikipedia.csv.1      77%[==============>     ] 742.10M  19.8MB/s    eta 17s    \rwikipedia.csv.1      77%[==============>     ] 746.24M  20.0MB/s    eta 17s    \rwikipedia.csv.1      78%[==============>     ] 750.64M  20.1MB/s    eta 17s    \rwikipedia.csv.1      78%[==============>     ] 754.47M  20.1MB/s    eta 16s    \rwikipedia.csv.1      79%[==============>     ] 758.55M  20.2MB/s    eta 16s    \rwikipedia.csv.1      79%[==============>     ] 762.69M  20.2MB/s    eta 16s    \rwikipedia.csv.1      80%[===============>    ] 766.55M  20.2MB/s    eta 16s    \rwikipedia.csv.1      80%[===============>    ] 770.87M  20.2MB/s    eta 16s    \rwikipedia.csv.1      80%[===============>    ] 774.77M  20.1MB/s    eta 14s    \rwikipedia.csv.1      81%[===============>    ] 779.22M  20.2MB/s    eta 14s    \rwikipedia.csv.1      81%[===============>    ] 782.87M  20.1MB/s    eta 14s    \rwikipedia.csv.1      82%[===============>    ] 787.19M  20.1MB/s    eta 14s    \rwikipedia.csv.1      82%[===============>    ] 791.12M  20.1MB/s    eta 14s    \rwikipedia.csv.1      83%[===============>    ] 794.91M  20.1MB/s    eta 12s    \rwikipedia.csv.1      83%[===============>    ] 799.35M  20.1MB/s    eta 12s    \rwikipedia.csv.1      83%[===============>    ] 803.18M  20.1MB/s    eta 12s    \rwikipedia.csv.1      84%[===============>    ] 807.52M  20.1MB/s    eta 12s    \rwikipedia.csv.1      84%[===============>    ] 811.24M  20.0MB/s    eta 12s    \rwikipedia.csv.1      85%[================>   ] 815.71M  20.0MB/s    eta 11s    \rwikipedia.csv.1      85%[================>   ] 820.16M  20.1MB/s    eta 11s    \rwikipedia.csv.1      86%[================>   ] 823.96M  20.0MB/s    eta 11s    \rwikipedia.csv.1      86%[================>   ] 828.26M  20.1MB/s    eta 11s    \rwikipedia.csv.1      86%[================>   ] 831.97M  20.1MB/s    eta 11s    \rwikipedia.csv.1      87%[================>   ] 836.36M  20.0MB/s    eta 9s     \rwikipedia.csv.1      87%[================>   ] 840.21M  20.0MB/s    eta 9s     \rwikipedia.csv.1      88%[================>   ] 844.19M  19.9MB/s    eta 9s     \rwikipedia.csv.1      88%[================>   ] 847.65M  19.8MB/s    eta 9s     \rwikipedia.csv.1      89%[================>   ] 852.55M  20.3MB/s    eta 9s     \rwikipedia.csv.1      89%[================>   ] 856.01M  20.0MB/s    eta 7s     \rwikipedia.csv.1      89%[================>   ] 860.59M  20.2MB/s    eta 7s     \rwikipedia.csv.1      90%[=================>  ] 864.12M  19.8MB/s    eta 7s     \rwikipedia.csv.1      90%[=================>  ] 868.19M  19.9MB/s    eta 7s     \rwikipedia.csv.1      91%[=================>  ] 872.33M  20.1MB/s    eta 7s     \rwikipedia.csv.1      91%[=================>  ] 876.07M  19.9MB/s    eta 6s     \rwikipedia.csv.1      91%[=================>  ] 879.96M  19.9MB/s    eta 6s     \rwikipedia.csv.1      92%[=================>  ] 884.13M  19.9MB/s    eta 6s     \rwikipedia.csv.1      92%[=================>  ] 888.15M  19.9MB/s    eta 6s     \rwikipedia.csv.1      93%[=================>  ] 892.38M  20.1MB/s    eta 6s     \rwikipedia.csv.1      93%[=================>  ] 896.29M  20.0MB/s    eta 4s     \rwikipedia.csv.1      94%[=================>  ] 900.51M  20.1MB/s    eta 4s     \rwikipedia.csv.1      94%[=================>  ] 904.35M  19.9MB/s    eta 4s     \rwikipedia.csv.1      94%[=================>  ] 908.57M  20.0MB/s    eta 4s     \rwikipedia.csv.1      95%[==================> ] 912.45M  19.9MB/s    eta 4s     \rwikipedia.csv.1      95%[==================> ] 916.74M  20.0MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 920.48M  20.0MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 925.04M  20.1MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 928.72M  20.1MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 933.22M  20.3MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 937.01M  20.2MB/s    eta 1s     \rwikipedia.csv.1      98%[==================> ] 941.29M  20.3MB/s    eta 1s     \rwikipedia.csv.1      98%[==================> ] 945.35M  20.3MB/s    eta 1s     \rwikipedia.csv.1      99%[==================> ] 949.26M  20.2MB/s    eta 1s     \rwikipedia.csv.1      99%[==================> ] 953.47M  20.2MB/s    eta 1s     \rwikipedia.csv.1      99%[==================> ] 956.41M  19.9MB/s    eta 0s     \rwikipedia.csv.1     100%[===================>] 956.99M  20.0MB/s    in 68s     \r\n\r\n2024-08-10 14:47:11 (14.1 MB/s) - ‘wikipedia.csv.1’ saved [1003477941/1003477941]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b0a872-fe88-4bc8-aa14-310761cc6d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ci predisponiamo il Dataframe spark di lavoro seguendo il seguente codice che va a leggere da un cluster AWS S3 il file csv e lo importa in un dataframe spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127e9378-64ce-496a-b5b3-a67b7c041575",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "N.b:\n",
    "Nel codice che segue, eseguiremo un campionamento stratificato del nostro dataset. Questo approccio è necessario per due ragioni principali poiché in fase di sviluppo stiamo lavorando in un ambiente Databricks Community, pertanto avremo accesso a risorse computazionali limitate.\n",
    "\n",
    "**Processo di campionamento**\n",
    "\n",
    "1. Estrarremo un campione del dataset originale.\n",
    "2. Utilizzeremo un metodo di campionamento stratificato basato sulla colonna \"categoria\".\n",
    "3. Questo significa che la distribuzione delle categorie nel nostro campione sarà proporzionalmente la stessa del dataset originale.\n",
    "\n",
    "Questo approccio ci permetterà di lavorare con un dataset più piccolo e gestibile, mantenendo allo stesso tempo la rappresentatività dei nostri dati originali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182dad7f-e652-4268-8a3a-945f654fa5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('/databricks/driver/wikipedia.csv')\n",
    "categoria_col = 'categoria'  \n",
    "\n",
    "# Riduciamo la size per agevolare il running nella versione community di databricks effettuando un campionamento stratificato (in modo che si mantiene bilanciato)\n",
    "dataset_sample, _ = train_test_split(dataset, test_size=0.1, stratify=dataset[categoria_col], random_state=42)\n",
    "\n",
    "spark_df_ = spark.createDataFrame(dataset_sample)\n",
    "spark_df_ = spark_df_.drop(\"Unnamed: 0\")\n",
    "spark_df_.write.mode(\"overwrite\").saveAsTable(\"wikipedia\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbd1eb8-422e-4b03-9c3e-87221253909c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mostriamo il contenuto del dataframe spark di lavoro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d4765-a347-4df9-a149-03448a7f3f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n|               title|             summary|           documents| categoria|\n+--------------------+--------------------+--------------------+----------+\n|jacques brel metr...|jacques brel is a...|jacques brel is a...| transport|\n|     pragya d. yadav|pragya d. yadav (...|pragya d. yadav (...|  research|\n|haplochromis sp. ...|haplochromis sp. ...|haplochromis sp. ...|      pets|\n|wilhelmina van in...|wilhelmina van in...|wilhelmina van in...|humanities|\n|           dash diet|the dash diet (di...|the dash diet (di...|  research|\n|charlie van gelderen|charlie van gelde...|charlie van gelde...| economics|\n|lockheed l-1049 s...|the lockheed l-10...|the lockheed l-10...|   science|\n|1989 nabisco mast...|stefan edberg def...|stefan edberg def...|    sports|\n|handheld pc magazine|smartphone & pock...|smartphone & pock...|technology|\n|       dancing ballz|dancing ballz is ...|dancing ballz is ...|technology|\n|     alumina limited|alumina limited i...|alumina limited i...|   finance|\n|    tropheus kasabae|tropheus kasabae ...|tropheus kasabae ...|      pets|\n|              deknni|deknni (also spel...|deknni (also spel...|   culture|\n|   qimei power plant|the qimei power p...|the qimei power p...|    energy|\n|watergardens rail...|watergardens rail...|watergardens rail...| transport|\n|haimen power station|haimen power stat...|haimen power stat...|    energy|\n|               meego|meego is a discon...|meego is a discon...|technology|\n|        wilberg mine|the wilberg mine ...|the wilberg mine ...|    energy|\n|grand eweng hydro...|the grand eweng h...|the grand eweng h...|    energy|\n|yizhuang railway ...|yizhuang railway ...|yizhuang railway ...| transport|\n+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark_df_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a51d85-a6dc-4ae7-8c28-fad5ddf523f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Implementeremo adesso una pipeline di Data Cleaning utilizzando SparkNLP. Il processo include:\n",
    "\n",
    "1. Normalizzazione del testo\n",
    "2. Lemmatizzazione\n",
    "3. Rimozione delle stopwords\n",
    "\n",
    "**Applicazione e Risultati**\n",
    "\n",
    "- DataFrame \"df_cleaned_summary\": Versione pulita della colonna \"summary\" su nuova colonna \"cleaned_text\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367eb18e-2e4c-446f-b989-b62ccb97723b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[OK!]\nstopwords_en download started this may take some time.\nApproximate size to download 2.9 KB\n\r[ | ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Per estrarre il testo pulito in una nuova colonna:\n",
    "from pyspark.sql.functions import concat_ws\n",
    "import re\n",
    "\n",
    "\n",
    "# Definiamo la pipeline di Spark NLP che pulisce il testo che agisce su \"summary\".\n",
    "\n",
    "# Crea un DocumentAssembler\n",
    "# Questo componente prende il testo grezzo e lo converte in un documento annotato con metadati di riferimento\n",
    "# setInputCol(\"text\"): specifica la colonna di input contenente il testo grezzo\n",
    "# setOutputCol(\"document\"): specifica la colonna di output per il documento annotato\n",
    "document_assembler = DocumentAssembler().setInputCol(\"summary\").setOutputCol(\"document\")\n",
    "\n",
    "# Crea un Tokenizer\n",
    "# Questo componente divide il testo in singole parole o token\n",
    "# setInputCols([\"document\"]): specifica la colonna di input (il documento annotato)\n",
    "# setOutputCol(\"token\"): specifica la colonna di output per i token\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# Crea un Normalizer\n",
    "# Questo componente normalizza il testo, ad esempio convertendolo in minuscolo\n",
    "# setInputCols([\"token\"]): specifica la colonna di input (i token)\n",
    "# setOutputCol(\"normalized\"): specifica la colonna di output per i token normalizzati\n",
    "# setLowercase(True): imposta la conversione in minuscolo\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setCleanupPatterns([\"[^\\w\\s]\"]) \\\n",
    "    .setLowercase(True) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# Crea un LemmatizerModel\n",
    "# Questo componente riduce le parole alla loro forma base (lemma)\n",
    "# pretrained(): carica un modello pre-addestrato per la lemmatizzazione\n",
    "# setInputCols([\"normalized\"]): specifica la colonna di input (i token normalizzati)\n",
    "# setOutputCol(\"lemma\"): specifica la colonna di output per i lemmi\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"normalized\"]).setOutputCol(\"lemma\")\n",
    "\n",
    "# Crea uno StopWordsCleaner\n",
    "# Questo componente rimuove le parole comuni (stopwords) che spesso non portano significato\n",
    "# pretrained(): carica una lista pre-definita di stopwords\n",
    "# setInputCols([\"lemma\"]): specifica la colonna di input (i lemmi)\n",
    "# setOutputCol(\"cleaned\"): specifica la colonna di output per le parole pulite\n",
    "stopwords_cleaner = StopWordsCleaner.pretrained().setInputCols([\"lemma\"]).setOutputCol(\"cleaned\")\n",
    "\n",
    "# Crea il pipeline\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer, \n",
    "    normalizer, \n",
    "    lemmatizer, \n",
    "    stopwords_cleaner\n",
    "])\n",
    "\n",
    "# Funzione per applicare la pipeline e pulire il testo\n",
    "def clean_text_spark_nlp(df, input_col=\"summary\", output_col=\"cleaned_text\"):\n",
    "    # Fit della pipeline \n",
    "    fitted_pipeline = nlp_pipeline.fit(df)\n",
    "    \n",
    "    # Applica il pipeline al DataFrame\n",
    "    result = fitted_pipeline.transform(df)\n",
    "\n",
    "    # Estraiamo il testo pulito in una nuova colonna\n",
    "    result_with_clean_text = result.withColumn(output_col, concat_ws(\" \", \"cleaned.result\"))\n",
    "\n",
    "    # Restituiamo il dataframe con la selezione delle colonne originali e la nuova colonna di testo pulito\n",
    "    return result_with_clean_text.select(df.columns + [output_col])\n",
    "\n",
    "# Applichiamo la funzione al tuo DataFrame\n",
    "df_cleaned_summary = clean_text_spark_nlp(spark_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8437175f-2ccb-4305-84ff-3ffd1f484623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Mostriamo adesso la struttura del df \"df_cleaned_summary\" con la colonna aggiunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d649c5a-07dc-44f5-b074-491e025de98f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+\n|               title|             summary|           documents| categoria|        cleaned_text|\n+--------------------+--------------------+--------------------+----------+--------------------+\n|jacques brel metr...|jacques brel is a...|jacques brel is a...| transport|jacques brel brus...|\n|     pragya d. yadav|pragya d. yadav (...|pragya d. yadav (...|  research|pragya yadav bear...|\n|haplochromis sp. ...|haplochromis sp. ...|haplochromis sp. ...|      pets|haplochromis sp c...|\n|wilhelmina van in...|wilhelmina van in...|wilhelmina van in...|humanities|wilhelmina van in...|\n|           dash diet|the dash diet (di...|the dash diet (di...|  research|dash diet dietary...|\n|charlie van gelderen|charlie van gelde...|charlie van gelde...| economics|charlie van gelde...|\n|lockheed l-1049 s...|the lockheed l-10...|the lockheed l-10...|   science|lockheed l1049 su...|\n|1989 nabisco mast...|stefan edberg def...|stefan edberg def...|    sports|stefan edberg def...|\n|handheld pc magazine|smartphone & pock...|smartphone & pock...|technology|smartphone pocket...|\n|       dancing ballz|dancing ballz is ...|dancing ballz is ...|technology|dance ballz mobil...|\n|     alumina limited|alumina limited i...|alumina limited i...|   finance|alumina limit aus...|\n|    tropheus kasabae|tropheus kasabae ...|tropheus kasabae ...|      pets|tropheus kasabae ...|\n|              deknni|deknni (also spel...|deknni (also spel...|   culture|deknni spell dekn...|\n|   qimei power plant|the qimei power p...|the qimei power p...|    energy|qimei power plant...|\n|watergardens rail...|watergardens rail...|watergardens rail...| transport|watergardens rail...|\n|haimen power station|haimen power stat...|haimen power stat...|    energy|haimen power stat...|\n|               meego|meego is a discon...|meego is a discon...|technology|meego discontinue...|\n|        wilberg mine|the wilberg mine ...|the wilberg mine ...|    energy|wilberg cottonwoo...|\n|grand eweng hydro...|the grand eweng h...|the grand eweng h...|    energy|grand eweng hydro...|\n|yizhuang railway ...|yizhuang railway ...|yizhuang railway ...| transport|yizhuang railway ...|\n+--------------------+--------------------+--------------------+----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f7d49c-d698-4af0-99ca-f3765e007160",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Ora che abbiamo completato la pulizia dei dati, procederemo con il primo step del nostro progetto. Calcoleremo una serie di statistiche importanti per ciascuna categoria presente nel nostro dataset. <br><br> \n",
    "\n",
    "Per ogni categoria in particolare andremo a calcolare :\n",
    "\n",
    "1. Numero totale di articoli  \n",
    "2. Numero medio di parole per articolo\n",
    "3. Numero di parole dell'articolo più lungo\n",
    "4. Numero di parole dell'articolo più corto\n",
    "5. Nuvola di parole (word cloud) rappresentativa\n",
    "   Questa visualizzazione mostrerà le parole più frequenti e significative per ciascuna categoria\n",
    "\n",
    "Queste analisi ci forniranno una panoramica dettagliata della struttura e del contenuto del nostro dataset, evidenziando le differenze tra le varie categorie in termini di lunghezza degli articoli e uso del linguaggio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689ff6d2-51a2-4d35-aa31-f1c919b3cc83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### a. Numero di articoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d60b32-e4d4-43cd-8865-06984cb0fe6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|  categoria|Num_articoli|\n+-----------+------------+\n|    finance|        5303|\n|   medicine|        7482|\n|   research|        7058|\n| technology|        6253|\n|     energy|        4393|\n|  transport|        7231|\n|   politics|         221|\n|    culture|        2033|\n|    science|        3506|\n| humanities|        5477|\n|  economics|        3889|\n|      trade|        3577|\n|     sports|        3515|\n|       pets|        4633|\n|engineering|        5715|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_num_articoli = spark.sql(\"SELECT categoria,COUNT(DISTINCT title) as Num_articoli FROM wikipedia GROUP BY categoria\") \n",
    "df_num_articoli.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28fba31-d025-4981-8cf1-1b1ff783423a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### b. Numero medio di parole utilizzate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4ecc4c-7160-40fb-a0ab-8fe5369c45b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In questa fase, calcoleremo il numero medio di parole utilizzate. <br>\n",
    "Per eseguire questa analisi, utilizzeremo uno strumento chiamato CountVectorizer.\n",
    "\n",
    "Il CountVectorizer è uno strumento potente per l'elaborazione del testo \n",
    "\n",
    "**Perché usiamo il CountVectorizer?**<br><br>\n",
    "\n",
    "1. Efficienza: Può gestire grandi volumi di testo in modo efficiente.\n",
    "2. Versatilità: Ci permette di ottenere facilmente il conteggio delle parole per documento.\n",
    "3. Preparazione per analisi future: I dati vettorizzati possono essere utilizzati per ulteriori analisi, come la classificazione del testo o il topic modeling.\n",
    "\n",
    "Nelle prossime celle di codice, definiremo e configureremo il CountVectorizer per la nostra analisi specifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c4f731-23cd-428a-a5eb-fe28c6e0dedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import udf, sum as spark_sum, split\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Tokenizziamo la colonna \"cleaned_text\" \n",
    "df_cleaned_summary = df_cleaned_summary.withColumn(\"words\", split(df_cleaned_summary.cleaned_text, \"\\\\s+\"))\n",
    "\n",
    "# Definiamo HashingTF sulla colonna \"words\"\n",
    "cv_model_summary = CountVectorizer(inputCol=\"words\", outputCol=\"word_vector\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559d8e6d-fd19-49df-a647-2fc93e30711d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Addestriamo il **CountVectorizer** per il df \"df_cleaned_summary\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74823efe-6b01-4a65-84ee-f7da6a3c0480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Addestro sul df per summary e vettorizzo\n",
    "cv_model_summary = cv_model_summary.fit(df_cleaned_summary)\n",
    "df_cleaned_summary_cont_vect = cv_model_summary.transform(df_cleaned_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979ced87-b0bd-4cec-8926-b41b4da95069",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Andiamo a vedere il tipo di colonna che mi ha generato nel dataframe che abbiamo denominato \"word_vector\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0c691c-f82d-47d0-968b-c11eca772e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- title: string (nullable = true)\n |-- summary: string (nullable = true)\n |-- documents: string (nullable = true)\n |-- categoria: string (nullable = true)\n |-- cleaned_text: string (nullable = false)\n |-- words: array (nullable = false)\n |    |-- element: string (containsNull = false)\n |-- word_vector: vector (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_summary_cont_vect.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab275634-846c-4ac8-bc7b-9f3e28034ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dopo aver applicato il CountVectorizer, abbiamo ottenuto una nuova colonna chiamata \"word_vector\". Questa colonna contiene vettori sparsi che rappresentano il conteggio delle parole per ogni documento. Ecco come procederemo per analizzare questi dati:\n",
    "\n",
    "1. Comprensione della Colonna \"word_vector\"\n",
    "- La colonna \"word_vector\" è di tipo \"vector\".\n",
    "- Ogni elemento di questo vettore rappresenta una parola unica nel vocabolario.\n",
    "- I valori diversi da zero in questo vettore indicano la presenza e la frequenza di una parola nel documento.\n",
    "\n",
    "2. Creazione di una Funzione Personalizzata (UDF)<br>\n",
    "Per contare efficacemente le parole uniche, creeremo una User Defined Function (UDF) che:\n",
    "- Prende come input il vettore di parole.\n",
    "- Conta quanti elementi nel vettore sono diversi da zero.\n",
    "- Restituisce questo conteggio, che rappresenta il numero di parole uniche nel documento.\n",
    "\n",
    "3. Applicazione della UDF e Calcolo delle Statistiche<br>\n",
    "Utilizzeremo questa funzione per:\n",
    "- Contare le parole uniche in ogni articolo.\n",
    "- Calcolare la media di parole uniche per categoria.\n",
    "\n",
    "Questo processo ci permetterà di ottenere:\n",
    "- Una visione dettagliata della ricchezza lessicale di ogni articolo.\n",
    "- Un'analisi comparativa della diversità di vocabolario tra le diverse categorie.\n",
    "\n",
    "Nelle prossime celle di codice, implementeremo questa funzione e la applicheremo al nostro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2257f9-484e-419c-972b-dfa59cff9a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, avg\n",
    "\n",
    "# Questa funzione è la User Defined Function (UDF) in PySpark che conta il numero di elementi non zero in un vettore sparso\n",
    "# La funzione viene definita con un decoratore che trasforma la funzione Python in una UDF di Spark.\n",
    "@udf(returnType=IntegerType())\n",
    "def count_non_zero(vector):\n",
    "    return int(vector.numNonzeros()) #è un metodo  per il vettore sparso che restituisce il numero di elementi non zero.\n",
    "\n",
    "# Calcola il conteggio delle parole uniche (creiamo il campo \"unique_words\")\n",
    "df_summary_result_with_count_def = df_cleaned_summary_cont_vect.withColumn(\"unique_words\", count_non_zero(\"word_vector\"))\n",
    "\n",
    "# Calcolo della media per categoria del n° individuato di parole utilizzate (creiamo il campo \"num_medio_words\")\n",
    "df_summary_count_def = df_summary_result_with_count_def.groupBy(\"categoria\").agg(avg(\"unique_words\")).alias(\"num_medio_words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e214b66-2bd0-4222-b8fb-fc839d785f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ecco il risultato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04fc753-8a65-44ae-b378-9e70cce59b42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n|  categoria| avg(unique_words)|\n+-----------+------------------+\n|    finance|49.270867519964995|\n|   medicine| 43.68383325981473|\n|   research| 42.43473928927267|\n| technology| 45.61974466211754|\n|     energy| 42.74187126741871|\n|  transport| 36.75627947789843|\n|   politics| 69.09058892584622|\n|    culture| 45.24638457418318|\n|    science|47.589231437255755|\n| humanities|40.370386452453324|\n|  economics| 49.51632047477745|\n|      trade|46.033991833131005|\n|     sports| 37.13927822536144|\n|       pets|31.504489524442967|\n|engineering|38.676668841052404|\n+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_summary_count_def.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c70ca87-d163-4b8c-967a-387875ffb979",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### c. Numero massimo di parole su articolo più lungo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afad0044-92c8-483c-8966-f6bff6add1db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ripartiamo dal df predisposto con il numero di parole per record :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "265ea886-3d78-4490-9bd9-dbcba49c542f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------+\n|               title|             summary|           documents| categoria|        cleaned_text|               words|         word_vector|unique_words|\n+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------+\n|jacques brel metr...|jacques brel is a...|jacques brel is a...| transport|jacques brel brus...|[jacques, brel, b...|(217564,[3,20,21,...|          44|\n|     pragya d. yadav|pragya d. yadav (...|pragya d. yadav (...|  research|pragya yadav bear...|[pragya, yadav, b...|(217564,[9,10,18,...|          76|\n|haplochromis sp. ...|haplochromis sp. ...|haplochromis sp. ...|      pets|haplochromis sp c...|[haplochromis, sp...|(217564,[41,89,16...|          14|\n|wilhelmina van in...|wilhelmina van in...|wilhelmina van in...|humanities|wilhelmina van in...|[wilhelmina, van,...|(217564,[18,34,45...|          25|\n|           dash diet|the dash diet (di...|the dash diet (di...|  research|dash diet dietary...|[dash, diet, diet...|(217564,[1,6,9,13...|         167|\n|charlie van gelderen|charlie van gelde...|charlie van gelde...| economics|charlie van gelde...|[charlie, van, ge...|(217564,[0,17,21,...|          46|\n|lockheed l-1049 s...|the lockheed l-10...|the lockheed l-10...|   science|lockheed l1049 su...|[lockheed, l1049,...|(217564,[1,6,14,1...|          31|\n|1989 nabisco mast...|stefan edberg def...|stefan edberg def...|    sports|stefan edberg def...|[stefan, edberg, ...|(217564,[26,69,12...|          18|\n|handheld pc magazine|smartphone & pock...|smartphone & pock...|technology|smartphone pocket...|[smartphone, pock...|(217564,[13,83,32...|          24|\n|       dancing ballz|dancing ballz is ...|dancing ballz is ...|technology|dance ballz mobil...|[dance, ballz, mo...|(217564,[31,36,51...|          37|\n|     alumina limited|alumina limited i...|alumina limited i...|   finance|alumina limit aus...|[alumina, limit, ...|(217564,[7,12,27,...|          17|\n|    tropheus kasabae|tropheus kasabae ...|tropheus kasabae ...|      pets|tropheus kasabae ...|[tropheus, kasaba...|(217564,[41,88,89...|          21|\n|              deknni|deknni (also spel...|deknni (also spel...|   culture|deknni spell dekn...|[deknni, spell, d...|(217564,[98,229,4...|          13|\n|   qimei power plant|the qimei power p...|the qimei power p...|    energy|qimei power plant...|[qimei, power, pl...|(217564,[5,55,157...|          16|\n|watergardens rail...|watergardens rail...|watergardens rail...| transport|watergardens rail...|[watergardens, ra...|(217564,[3,20,21,...|          20|\n|haimen power station|haimen power stat...|haimen power stat...|    energy|haimen power stat...|[haimen, power, s...|(217564,[3,5,58,4...|           6|\n|               meego|meego is a discon...|meego is a discon...|technology|meego discontinue...|[meego, discontin...|(217564,[13,22,42...|         111|\n|        wilberg mine|the wilberg mine ...|the wilberg mine ...|    energy|wilberg cottonwoo...|[wilberg, cottonw...|(217564,[1,5,7,17...|          46|\n|grand eweng hydro...|the grand eweng h...|the grand eweng h...|    energy|grand eweng hydro...|[grand, eweng, hy...|(217564,[1,3,5,16...|          28|\n|yizhuang railway ...|yizhuang railway ...|yizhuang railway ...| transport|yizhuang railway ...|[yizhuang, railwa...|(217564,[3,24,28,...|          26|\n+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_summary_result_with_count_def.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5c502c-b207-419f-a886-f25059f25a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ora procederemo a calcolare il numero massimo di parole utilizzate tra tutti gli articoli per ciascuna categoria. Questo ci darà un'idea della lunghezza dell'articolo più verboso in ogni categoria.\n",
    "\n",
    "**Procedimento**\n",
    "1. Utilizzeremo la colonna che abbiamo creato precedentemente, contenente il conteggio delle parole uniche per ogni articolo.\n",
    "2. Raggrupperemo i dati per categoria.\n",
    "3. Per ogni gruppo (categoria), troveremo il valore massimo del conteggio di parole.\n",
    "\n",
    "**Significato del Risultato**\n",
    "- Questo calcolo ci mostrerà la \"diversità lessicale massima\" all'interno di ciascuna categoria.\n",
    "- Ci permetterà di identificare quali categorie tendono ad avere articoli più lunghi o più ricchi di vocabolario.\n",
    "- Potrebbe rivelare differenze interessanti tra le categorie in termini di complessità o dettaglio degli articoli.\n",
    "\n",
    "Nelle prossime celle di codice, implementeremo questa analisi utilizzando le funzioni di aggregazione di PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a3537c-d4a2-4f84-8a05-d32dceaf9cf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Calcolo della media per categoria del n° individuato di parole utilizzate \n",
    "df_num_max_words_summary = df_summary_result_with_count_def.groupBy(\"categoria\").agg(max(\"unique_words\")).alias(\"max_words\")\n",
    "df_num_max_words_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae38247-0582-4b3e-aee2-98aa4d1e2d58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### d. Numero minimo di parole su articolo più corto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5026ac04-f8c3-42f1-ab68-385c255f49c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Analogamente per quanto fatto sul massimo numero di parole per categoria, ora ci concentreremo sul calcolo del numero minimo di parole uniche utilizzate per ciascuna categoria. Questa analisi ci permetterà di identificare l' articolo più conciso o sintetico in ogni categoria.\n",
    "\n",
    "## Procedimento\n",
    "1. Utilizzeremo la colonna che abbiamo creato precedentemente, contenente il conteggio delle parole uniche per ogni articolo.\n",
    "2. Raggrupperemo i dati per categoria.\n",
    "3. Per ogni gruppo (categoria), troveremo il valore minimo del conteggio di parole.\n",
    "\n",
    "## Significato del Risultato\n",
    "- Questo calcolo ci mostrerà la \"diversità lessicale minima\" all'interno di ciascuna categoria.\n",
    "- Ci permetterà di identificare quali categorie tendono ad avere articoli più brevi o più sintetici.\n",
    "- Potrebbe rivelare differenze interessanti tra le categorie in termini di concisione o stile di scrittura.\n",
    "- Potrebbe anche aiutare a identificare potenziali outlier o articoli che potrebbero necessitare di ulteriore elaborazione o verifica.\n",
    "\n",
    "Nelle prossime celle di codice, implementeremo questa analisi utilizzando le funzioni di aggregazione di PySpark, concentrandoci sulla ricerca del valore minimo per ogni categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a926e352-8e20-4548-a1bb-94c50e260292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min \n",
    "\n",
    "# Calcolo della media per categoria del n° individuato di parole utilizzate \n",
    "df_num_min_words_summary = df_summary_result_with_count_def.groupBy(\"categoria\").agg(min(\"unique_words\")).alias(\"min_words\")\n",
    "df_num_min_words_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be0c908-a14d-438f-94a1-ca48db40c321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### e. Nuvola parole rappresentativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19706647-4ef7-40e2-a70b-4c455500a48f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Per individuare le parole più significative e rappresentative di ciascuna categoria, utilizzeremo la tecnica TF-IDF (Term Frequency-Inverse Document Frequency). Questa metodologia statistica è ampiamente utilizzata nell'elaborazione del linguaggio naturale e nel recupero delle informazioni.\n",
    "\n",
    "**Cos'è TF-IDF?**\n",
    "TF-IDF è una misura statistica che valuta l'importanza di una parola in un documento all'interno di una collezione o corpus. Si compone di due parti:\n",
    "\n",
    "1. TF (Term Frequency): Misura quanto frequentemente una parola appare in un documento.\n",
    "2. IDF (Inverse Document Frequency): Misura l'importanza della parola nell'intero corpus.\n",
    "\n",
    "Il punteggio TF-IDF aumenta proporzionalmente al numero di volte in cui una parola appare nel documento, ma è compensato dalla frequenza della parola nel corpus, il che aiuta a controllare il fatto che alcune parole sono generalmente più comuni di altre.\n",
    "\n",
    "**Perché usiamo TF-IDF?**\n",
    "\n",
    "1. Rilevanza: TF-IDF identifica parole che sono importanti in un particolare documento rispetto all'intero corpus.\n",
    "2. Oggettività: Fornisce una misura statistica dell'importanza delle parole, riducendo la soggettività.\n",
    "3. Contestualizzazione: Considera non solo la frequenza delle parole, ma anche la loro unicità nel contesto più ampio.\n",
    "\n",
    "Nelle prossime celle di codice, implementeremo il calcolo TF-IDF e l'estrazione delle parole più rappresentative per ciascuna categoria utilizzando PySpark e le sue librerie di machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d848167a-651c-4b14-b19b-2dc23bf1316b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "## Addestro sul df per summary e vettorizzo\n",
    "#cv_model_summary = cv.fit(df_cleaned_summary)\n",
    "#df_cleaned_summary_cont_vect = cv_model_summary.transform(df_cleaned_summary)\n",
    "\n",
    "\n",
    "# Esecuzione dell'analisi\n",
    "vocabulary = cv_model_summary.vocabulary\n",
    "\n",
    "# Calcolo dell'IDF\n",
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_cleaned_summary_cont_vect)\n",
    "df_tfidf = idfModel.transform(df_cleaned_summary_cont_vect)\n",
    "\n",
    "\n",
    "# Convertiamo il vettore TF-IDF in un array\n",
    "df_tfidf_array = df_tfidf.select(\n",
    "    \"categoria\",\n",
    "    vector_to_array(\"features\").alias(\"tfidf_array\") \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cb4b85-2e6c-4d41-a851-f91e2891a984",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Andiamo a verificare il primo output relativo all'implementazione TF-IDF e relativa conversione in array (vedi dataframe df_tfidf_array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad9bd37-8064-45a2-b7fb-c6ce580fb602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tfidf_array.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055c3b3b-46a4-4eea-a202-c81614f4f422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creiamo un DataFrame del vocabolario\n",
    "vocab_df = spark.createDataFrame(\n",
    "    [(i, word) for i, word in enumerate(vocabulary)], [\"index\", \"word\"])\n",
    "\n",
    "# Esplodiamo l'array TF-IDF andando a determinare il valore per singola word\n",
    "df_exploded = df_tfidf_array.select(\n",
    "    \"categoria\",\n",
    "    F.explode(F.arrays_zip(\n",
    "        F.array([F.lit(i) for i in range(len(vocabulary))]), # Definizione dell'indice da 0 a len(vocabulary)-1\n",
    "        \"tfidf_array\"                                        # Definizione della singola parola\n",
    "    )).alias(\"idx_tfidf\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaeefaf9-5419-42dc-b2e4-3fab6863c3b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nel codice precedente abbiamo compiuto due operazioni principali:\n",
    "\n",
    "1. Definizione del vocabolario:\n",
    "   Abbiamo creato un \"vocabolario\" che contiene l'elenco completo di tutte le parole identificate dal **CountVectorizer**. Questo vocabolario ci permette di associare ogni parola a un indice numerico specifico.\n",
    "\n",
    "2. Creazione del DataFrame \"df_exploded\":\n",
    "   Abbiamo generato un nuovo DataFrame chiamato \"df_exploded\". In questo DataFrame, abbiamo organizzato i valori TF-IDF di ogni parola per ciascuna categoria. Ogni riga di questo DataFrame rappresenta una singola parola in una specifica categoria, con il suo corrispondente valore TF-IDF.\n",
    "\n",
    "Nelle fasi successive del codice, procederemo come segue:\n",
    "\n",
    "1. Filtraggio dei risultati:\n",
    "   Selezioneremo solo i record con un punteggio TF-IDF maggiore di zero. Questo ci permetterà di concentrarci esclusivamente sulle parole che hanno una rilevanza significativa all'interno di ciascuna categoria.\n",
    "\n",
    "2. Ordinamento e selezione:\n",
    "   Ordineremo i risultati in base al punteggio TF-IDF, dal più alto al più basso. Questo ci consentirà di identificare le parole più rappresentative per ogni categoria.\n",
    "\n",
    "3. Visualizzazione dei risultati:\n",
    "   Mostreremo le prime 5 parole più significative per ogni categoria. Queste parole saranno quelle con i punteggi TF-IDF più elevati, rappresentando così i termini più caratteristici e distintivi di ciascuna categoria.\n",
    "\n",
    "Questo processo ci permetterà di identificare e visualizzare le parole chiave più rilevanti per ogni categoria, basandoci sulla loro importanza calcolata attraverso il metodo TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb907cf-92a6-4d1d-ba23-7397ed673e50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Andiamo ora a mostrare il DF \"df_exploded\" che conterrà la colonna \"categoria\" e l'elenco dei valori TF-IDF con indice del vocabolario della parola di riferimento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c32e362-1e22-433b-8279-d704fd5a21ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2beab47-b68c-45b8-8f22-a5ae9387e6c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Andiamo adesso a separare nella colonna \"idx_tfidf\" l'indice e lo score che utilizzeremo per individuare le parole più rappresentative della categorie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cf5dc8-e8c3-499f-87b3-92502119d814",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Uniamo con il vocabolario e filtriamo i valori positivi\n",
    "df_words = df_exploded.select(\n",
    "    \"categoria\",\n",
    "    F.col(\"idx_tfidf.0\").alias(\"index\"),\n",
    "    F.col(\"idx_tfidf.tfidf_array\").alias(\"tfidf_score\")\n",
    ").filter(F.col(\"tfidf_score\") > 0) \\\n",
    "    .join(vocab_df, \"index\") \\\n",
    "    .select(\"categoria\", \"word\", \"tfidf_score\").distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a655f7-e2f3-4953-9092-451e0e6dddfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ottenuto il DF desiderato andiamo ora per categoria a mostrare le prime 5 parole più rappresentative ordinando in modalità decrescente gli score individuati e prendendo i primi cinque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e4f7cf-e981-4c00-a64a-ee5bfff8d067",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Troviamo le top n parole per ogni categoria\n",
    "window = Window.partitionBy(\"categoria\").orderBy(F.col(\"tfidf_score\").desc())\n",
    "\n",
    "df_top_words = df_words.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "                .filter(F.col(\"rank\") <= 5) \\\n",
    "                .select(\"categoria\", \"word\", \"tfidf_score\")\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Top 5 parole più rappresentative per categoria:\")\n",
    "df_top_words.orderBy(\"categoria\", F.col(\"tfidf_score\").desc()).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Progetto_EDA",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
